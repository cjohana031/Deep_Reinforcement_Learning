{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-28T02:06:24.622086Z",
     "start_time": "2025-08-28T02:06:24.614223Z"
    }
   },
   "source": [
    "import gymnasium as gym\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from collections import deque\n",
    "import random\n",
    "from pathlib import Path\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "import webbrowser"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lunar landing problem\n",
    "Game general descripcion\n",
    "\n",
    "- Game specific description\n",
    "- Action space\n",
    "- Observation space\n",
    "- Rewards\n",
    "- Final states"
   ],
   "id": "4da7cab4d697c16f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T02:06:24.651914Z",
     "start_time": "2025-08-28T02:06:24.640295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SEED = 42\n",
    "class Environment:\n",
    "    def __init__(self, model_id: str, render_mode=None, seed=None):\n",
    "        self.env = gym.make(model_id, render_mode=render_mode)\n",
    "        self.seed = seed\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "\n",
    "    def reset(self):\n",
    "        observation, info = self.env.reset(seed=self.seed)\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def render(self):\n",
    "        return self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    def get_action_space_size(self):\n",
    "        return self.action_space.n\n",
    "\n",
    "    def get_observation_space_shape(self):\n",
    "        return self.observation_space.shape"
   ],
   "id": "f59acd21fd8d46f4",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T02:06:24.710686Z",
     "start_time": "2025-08-28T02:06:24.703975Z"
    }
   },
   "cell_type": "code",
   "source": "lunar_landing_env = Environment(model_id=\"LunarLander-v3\", render_mode=\"human\", seed=SEED)",
   "id": "5002a3b2086b4a1c",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T02:06:24.775876Z",
     "start_time": "2025-08-28T02:06:24.767463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Action space: {lunar_landing_env.action_space}\")\n",
    "print(f\"Observation space: {lunar_landing_env.observation_space}\")"
   ],
   "id": "2f4eef594220043c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(4)\n",
      "Observation space: Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
      "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
      "  1.         1.       ], (8,), float32)\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we can observe in the observation space, we have a 8 dimensional vector where its values are almost in the same\n",
    "scale."
   ],
   "id": "95473e01f7defe95"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Modeling: DQN\n",
    "\n",
    "To solve this problem he have chose DQN algorithm because:\n",
    "\n",
    "## Tech stack for modeling\n",
    "\n",
    "Pytorch...\n",
    "\n",
    "Architecture diagram\n",
    "\n",
    "Layer sizes experiments... and results"
   ],
   "id": "8e8ad24a1ce94dcf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T02:06:24.912706Z",
     "start_time": "2025-08-28T02:06:24.903559Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_sizes=[128, 128]):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        input_dim = state_dim\n",
    "\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_dim, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_dim = hidden_size\n",
    "\n",
    "        layers.append(nn.Linear(input_dim, action_dim))\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.network(state)"
   ],
   "id": "34ae49df7c341578",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training process\n",
    "\n",
    "- Target network and q network, why\n",
    "- Actualization strategy, why\n",
    "- Experience replay, why (maybe pondered experience replay, why)\n",
    "- Epsilon-greedy strategy, why"
   ],
   "id": "8f7fbcf9465e1311"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Replay buffer",
   "id": "e72a6c11d501d6b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T02:06:24.988401Z",
     "start_time": "2025-08-28T02:06:24.977032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device='cpu'):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.device = device\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*batch)\n",
    "\n",
    "        state = torch.FloatTensor(np.array(state)).to(self.device)\n",
    "        action = torch.LongTensor(action).to(self.device)\n",
    "        reward = torch.FloatTensor(reward).to(self.device)\n",
    "        next_state = torch.FloatTensor(np.array(next_state)).to(self.device)\n",
    "        done = torch.FloatTensor(done).to(self.device)\n",
    "\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ],
   "id": "2a4515bed05feec2",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# DQN Agent and DQN Agent Updater",
   "id": "79cedce08a5d5dab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T02:06:25.080727Z",
     "start_time": "2025-08-28T02:06:25.038781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQNAgent:\n",
    "    def __init__(\n",
    "            self,\n",
    "            state_dim,\n",
    "            action_dim,\n",
    "            model_dir='models/dqn',\n",
    "            epsilon=1.0,\n",
    "    ):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.model_dir = Path(model_dir)\n",
    "        self.model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            device = \"cuda\"\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = \"mps\"\n",
    "        else:\n",
    "            device = \"cpu\"\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.q_network = DQNNetwork(state_dim, action_dim, hidden_sizes=[256, 256]).to(self.device)\n",
    "        self.target_network = DQNNetwork(state_dim, action_dim, hidden_sizes=[256, 256]).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "\n",
    "    def act(self, state, training=True):\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "    def save(self, episode=None):\n",
    "        filename = f\"dqn_episode_{episode}.pth\" if episode else \"dqn_final.pth\"\n",
    "        filepath = self.model_dir / filename\n",
    "\n",
    "        checkpoint = {\n",
    "            'q_network_state_dict': self.q_network.state_dict(),\n",
    "            'target_network_state_dict': self.target_network.state_dict(),\n",
    "            'episode': episode\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "        if episode is None:\n",
    "            best_path = self.model_dir / \"dqn_best.pth\"\n",
    "            torch.save(checkpoint, best_path)\n",
    "\n",
    "    def load(self, filepath=None):\n",
    "        if filepath is None:\n",
    "            filepath = self.model_dir / \"dqn_best.pth\"\n",
    "            if not filepath.exists():\n",
    "                filepath = self.model_dir / \"dqn_final.pth\"\n",
    "\n",
    "        if not Path(filepath).exists():\n",
    "            print(f\"No model found at {filepath}\")\n",
    "            return False\n",
    "\n",
    "        checkpoint = torch.load(filepath, map_location=self.device)\n",
    "        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])\n",
    "        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])\n",
    "\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "        return True\n",
    "\n",
    "class DQNUpdater:\n",
    "    def __init__(\n",
    "            self,\n",
    "            agent: DQNAgent,\n",
    "            target_update_freq=10,\n",
    "            buffer_size=10000,\n",
    "            batch_size=64,\n",
    "            lr=1e-3,\n",
    "            gamma=0.99,\n",
    "            epsilon_min=0.01,\n",
    "            epsilon_decay=0.995,\n",
    "            device='cpu'\n",
    "    ):\n",
    "        self.agent = agent\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        self.optimizer = optim.Adam(self.agent.q_network.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.update_counter = 0\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size, self.device)\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "        current_q_values = self.agent.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.agent.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + self.gamma * next_q_values * (1 - dones)  # TODO: check it\n",
    "\n",
    "        loss = self.loss_fn(current_q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.agent.q_network.parameters(), 1.0)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.update_counter += 1\n",
    "        if self.update_counter % self.target_update_freq == 0:\n",
    "            self.agent.target_network.load_state_dict(self.agent.q_network.state_dict())\n",
    "\n",
    "        if self.agent.epsilon > self.epsilon_min:\n",
    "            self.agent.epsilon *= self.epsilon_decay\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def save(self, episode=None):\n",
    "        checkpoint = {\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'update_counter': self.update_counter,\n",
    "            'episode': episode\n",
    "        }\n",
    "\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        self.update_counter = checkpoint.get('update_counter', 0)\n",
    "\n",
    "        # TODO: save agent updater"
   ],
   "id": "2c585b6a5d3e22e2",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T02:06:25.122014Z",
     "start_time": "2025-08-28T02:06:25.099692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, log_dir='logs', tensorboard_dir='runs'):\n",
    "        self.log_dir = log_dir\n",
    "        self.history = {\n",
    "            'episodes': [],\n",
    "            'rewards': [],\n",
    "            'steps': [],\n",
    "            'timestamps': []\n",
    "        }\n",
    "\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "\n",
    "        # Initialize TensorBoard writer\n",
    "        run_name = f\"DQN_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        self.tensorboard_dir = os.path.join(tensorboard_dir, run_name)\n",
    "        self.writer = SummaryWriter(self.tensorboard_dir)\n",
    "\n",
    "    def log_episode(self, episode, reward, steps):\n",
    "        self.history['episodes'].append(episode)\n",
    "        self.history['rewards'].append(reward)\n",
    "        self.history['steps'].append(steps)\n",
    "        self.history['timestamps'].append(datetime.now().isoformat())\n",
    "\n",
    "        # Log to TensorBoard\n",
    "        self.writer.add_scalar('Episode/Reward', reward, episode)\n",
    "        self.writer.add_scalar('Episode/Steps', steps, episode)\n",
    "\n",
    "        # Log moving averages\n",
    "        if len(self.history['rewards']) >= 10:\n",
    "            avg_10 = np.mean(self.history['rewards'][-10:])\n",
    "            self.writer.add_scalar('Average/Reward_10ep', avg_10, episode)\n",
    "\n",
    "        if len(self.history['rewards']) >= 100:\n",
    "            avg_100 = np.mean(self.history['rewards'][-100:])\n",
    "            self.writer.add_scalar('Average/Reward_100ep', avg_100, episode)\n",
    "\n",
    "    def save_history(self, filename=None):\n",
    "        if filename is None:\n",
    "            filename = f\"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "        filepath = os.path.join(self.log_dir, filename)\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.history, f, indent=2)\n",
    "\n",
    "        print(f\"Training history saved to {filepath}\")\n",
    "\n",
    "    def get_history(self):\n",
    "        return self.history\n",
    "\n",
    "    def save(self, filepath):\n",
    "        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "        np.savez(filepath, **self.history)\n",
    "        print(f\"Training history saved to {filepath}\")\n",
    "\n",
    "    def get_statistics(self, last_n=100):\n",
    "        if len(self.history['rewards']) < last_n:\n",
    "            last_n = len(self.history['rewards'])\n",
    "\n",
    "        if last_n == 0:\n",
    "            return {}\n",
    "\n",
    "        recent_rewards = self.history['rewards'][-last_n:]\n",
    "        recent_steps = self.history['steps'][-last_n:]\n",
    "\n",
    "        stats = {\n",
    "            'mean_reward': sum(recent_rewards) / len(recent_rewards),\n",
    "            'max_reward': max(recent_rewards),\n",
    "            'min_reward': min(recent_rewards),\n",
    "            'mean_steps': sum(recent_steps) / len(recent_steps),\n",
    "            'total_episodes': len(self.history['episodes'])\n",
    "        }\n",
    "\n",
    "        return stats\n",
    "\n",
    "    def log_training_metrics(self, episode, loss=None, epsilon=None, eval_reward=None):\n",
    "        \"\"\"Log additional training metrics to TensorBoard\"\"\"\n",
    "        if loss is not None:\n",
    "            self.writer.add_scalar('Training/Loss', loss, episode)\n",
    "        if epsilon is not None:\n",
    "            self.writer.add_scalar('Training/Epsilon', epsilon, episode)\n",
    "        if eval_reward is not None:\n",
    "            self.writer.add_scalar('Evaluation/Reward', eval_reward, episode)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close the TensorBoard writer\"\"\"\n",
    "        self.writer.close()\n",
    "        print(f\"TensorBoard logs saved to {self.tensorboard_dir}\")\n",
    "        return self.tensorboard_dir"
   ],
   "id": "edfadc1d57f27ec2",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training loop",
   "id": "c537bdde80192ae7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-28T02:06:25.222519Z",
     "start_time": "2025-08-28T02:06:25.186069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_dqn(\n",
    "    episodes=1000,\n",
    "    max_steps=1000,\n",
    "    save_freq=100,\n",
    "    eval_freq=50,\n",
    "    eval_episodes=10,\n",
    "    seed=42\n",
    "):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    env = Environment(\"LunarLander-v3\", render_mode=None, seed=seed)\n",
    "    eval_env = Environment(\"LunarLander-v3\", render_mode=None, seed=seed + 1000)\n",
    "\n",
    "    state_dim = env.get_observation_space_shape()[0]\n",
    "    action_dim = env.get_action_space_size()\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    agent = DQNAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "    )\n",
    "\n",
    "    agent_updater = DQNUpdater(\n",
    "        agent=agent,\n",
    "        lr=5e-4,\n",
    "        gamma=0.99,\n",
    "        epsilon_min=0.01,\n",
    "        epsilon_decay=0.995,\n",
    "        buffer_size=50000,\n",
    "        batch_size=64,\n",
    "        target_update_freq=100,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    logger = Logger()\n",
    "    best_avg_reward = -float('inf')\n",
    "    training_info = {\n",
    "        'start_time': datetime.now().isoformat(),\n",
    "        'episodes': episodes,\n",
    "        'seed': seed,\n",
    "        'device': agent.device\n",
    "    }\n",
    "\n",
    "    print(f\"Starting DQN training on {agent.device}\")\n",
    "    print(f\"State dimension: {state_dim}, Action dimension: {action_dim}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        losses = []\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action = agent.act(state, training=True)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            agent_updater.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "            loss = agent_updater.update()\n",
    "            if loss is not None:\n",
    "                losses.append(loss)\n",
    "\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        logger.log_episode(episode, total_reward, steps)\n",
    "\n",
    "        avg_loss = np.mean(losses) if losses else 0\n",
    "\n",
    "        # Log additional metrics to TensorBoard\n",
    "        logger.log_training_metrics(\n",
    "            episode=episode,\n",
    "            loss=avg_loss if losses else None,\n",
    "            epsilon=agent.epsilon,\n",
    "        )\n",
    "\n",
    "        if episode % 10 == 0:\n",
    "            avg_reward_100 = np.mean(logger.get_history()['rewards'][-100:]) if episode >= 99 else np.mean(logger.get_history()['rewards'])\n",
    "            print(f\"Episode {episode:4d} | Reward: {total_reward:7.2f} | Steps: {steps:3d} | \"\n",
    "                  f\"Avg100: {avg_reward_100:7.2f} | Loss: {avg_loss:.4f} | ε: {agent.epsilon:.3f}\")\n",
    "\n",
    "        if episode % eval_freq == 0 and episode > 0:\n",
    "            eval_rewards = []\n",
    "            for _ in range(eval_episodes):\n",
    "                state, _ = eval_env.reset()\n",
    "                eval_reward = 0\n",
    "                for _ in range(max_steps):\n",
    "                    action = agent.act(state, training=False)\n",
    "                    state, reward, terminated, truncated, _ = eval_env.step(action)\n",
    "                    eval_reward += reward\n",
    "                    if terminated or truncated:\n",
    "                        break\n",
    "                eval_rewards.append(eval_reward)\n",
    "\n",
    "            avg_eval_reward = np.mean(eval_rewards)\n",
    "            print(f\"  [EVAL] Average reward over {eval_episodes} episodes: {avg_eval_reward:.2f}\")\n",
    "\n",
    "            # Log evaluation results to TensorBoard\n",
    "            logger.log_training_metrics(episode=episode, eval_reward=avg_eval_reward)\n",
    "\n",
    "            if avg_eval_reward > best_avg_reward:\n",
    "                best_avg_reward = avg_eval_reward\n",
    "                agent.save()\n",
    "                print(f\"  [SAVE] New best model! Average reward: {best_avg_reward:.2f}\")\n",
    "\n",
    "        if episode % save_freq == 0 and episode > 0:\n",
    "            agent.save(episode)\n",
    "\n",
    "    agent.save()\n",
    "\n",
    "    training_info['end_time'] = datetime.now().isoformat()\n",
    "    training_info['final_avg_reward'] = float(np.mean(logger.get_history()['rewards'][-100:]))\n",
    "    training_info['best_avg_reward'] = float(best_avg_reward)\n",
    "\n",
    "    with open('models/dqn/training_info.json', 'w') as f:\n",
    "        json.dump(training_info, f, indent=2)\n",
    "\n",
    "    logger.save('logs/dqn/training_history.npz')\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Training completed!\")\n",
    "    print(f\"Final average reward (last 100 episodes): {training_info['final_avg_reward']:.2f}\")\n",
    "    print(f\"Best evaluation average reward: {best_avg_reward:.2f}\")\n",
    "    print(f\"Models saved in: models/dqn/\")\n",
    "\n",
    "    env.close()\n",
    "    eval_env.close()\n",
    "\n",
    "    # Close TensorBoard writer and get log directory\n",
    "    tensorboard_dir = logger.close()\n",
    "\n",
    "    # Launch TensorBoard\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Launching TensorBoard...\")\n",
    "    print(f\"TensorBoard will show results from: {tensorboard_dir}\")\n",
    "\n",
    "    try:\n",
    "        # Start TensorBoard process\n",
    "        tb_process = subprocess.Popen(\n",
    "            [\"tensorboard\", \"--logdir\", tensorboard_dir, \"--port\", \"6006\"],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "\n",
    "        # Give TensorBoard time to start\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Open in browser\n",
    "        url = \"http://localhost:6006\"\n",
    "        print(f\"Opening TensorBoard at {url}\")\n",
    "        webbrowser.open(url)\n",
    "\n",
    "        print(\"\\nTensorBoard is running! Press Ctrl+C to stop it.\")\n",
    "        print(\"You can also manually visit: http://localhost:6006\")\n",
    "\n",
    "        # Keep the process running\n",
    "        try:\n",
    "            tb_process.wait()\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nStopping TensorBoard...\")\n",
    "            tb_process.terminate()\n",
    "            tb_process.wait(timeout=5)\n",
    "    except FileNotFoundError:\n",
    "        print(\"TensorBoard not found. Please install it with: pip install tensorboard\")\n",
    "        print(f\"You can manually view the logs by running: tensorboard --logdir {tensorboard_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error launching TensorBoard: {e}\")\n",
    "        print(f\"You can manually view the logs by running: tensorboard --logdir {tensorboard_dir}\")\n",
    "\n",
    "    return logger.get_history()"
   ],
   "id": "360f420b7004edb1",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-08-28T02:06:25.251417Z"
    }
   },
   "cell_type": "code",
   "source": "train_dqn(1000)",
   "id": "67ca138ae5017ced",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting DQN training on cuda\n",
      "State dimension: 8, Action dimension: 4\n",
      "--------------------------------------------------\n",
      "Episode    0 | Reward: -307.65 | Steps: 107 | Avg100: -307.65 | Loss: 1.9213 | ε: 0.802\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "95e77eb34616e234"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
