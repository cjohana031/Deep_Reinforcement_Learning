# Experiment explanation

# Why are we using dqn (or the chosen algorithm )

# Reports of rewards while training

# Report of final rewards on 10 episodes

# To include:

1. Trained model (weights)
2. Model parameters
3. Notebook with the e2e process (environment setup, agent setup, inference configuration, etc)
4. Libraries
5. Hardware description 

## Results summary

1. Average reward (10 epochs after training)
2. Training time
3. Reward while training/exploitation per each epoch
4. Conclusion about implemented models capabilities 
5. Two short videos with the agent while:
- Training 
- After training