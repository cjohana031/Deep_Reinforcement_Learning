{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducci√≥n\n",
    "\n",
    "Este notebook implementa un **Soft Actor-Critic (SAC)** para resolver la tarea **Adroit Hand Door** del conjunto gymnasium-robotics utilizando el entorno **AdroitHandDoor-v1**.\n",
    "El objetivo es entrenar un agente que aprenda a **abrir una puerta** manipulando la manija con una mano rob√≥tica articulada de 28 grados de libertad.\n",
    "\n",
    "---\n",
    "\n",
    "# El Problema: Adroit Hand Door\n",
    "\n",
    "**Adroit Hand Door** es una tarea de manipulaci√≥n rob√≥tica compleja donde el agente debe:\n",
    "\n",
    "- Controlar una mano rob√≥tica de Shadow con 24 grados de libertad + brazo de 4 grados de libertad\n",
    "- Localizar y agarrar la manija de la puerta\n",
    "- Desbloquear el pestillo que tiene fricci√≥n significativa y sesgo de torque\n",
    "- Abrir la puerta completamente hasta que toque el tope del otro lado\n",
    "- Manejar la posici√≥n randomizada de la puerta en cada episodio\n",
    "\n",
    "---\n",
    "\n",
    "## Especificaciones T√©cnicas\n",
    "\n",
    "### Espacio de Acciones\n",
    "\n",
    "- **Tipo:** Continuo Box(-1.0, 1.0, (28,), float32)\n",
    "- **Dimensiones:** 28 acciones correspondientes a posiciones angulares absolutas\n",
    "  - Acciones 0-3: Movimiento del brazo (traslaci√≥n lineal, movimientos angulares)\n",
    "  - Acciones 4-5: Articulaciones de la mu√±eca (desviaci√≥n radial/ulnar, flexi√≥n/extensi√≥n)\n",
    "  - Acciones 6-9: Dedo √≠ndice (MCP, PIP, DIP)\n",
    "  - Acciones 10-13: Dedo medio (MCP, PIP, DIP)\n",
    "  - Acciones 14-17: Dedo anular (MCP, PIP, DIP)\n",
    "  - Acciones 18-22: Dedo me√±ique (CMC, MCP, PIP, DIP)\n",
    "  - Acciones 23-27: Pulgar (CMC, MCP, IP)\n",
    "\n",
    "---\n",
    "\n",
    "### Espacio de Observaciones\n",
    "\n",
    "- **Tipo:** Box(-inf, inf, (39,), float64)\n",
    "- **Dimensiones:** 39 observaciones que incluyen:\n",
    "  - Posiciones angulares de las articulaciones de la mano (27 elementos)\n",
    "  - Posici√≥n angular del pestillo de la puerta (1 elemento)\n",
    "  - Posici√≥n angular de la bisagra de la puerta (1 elemento)\n",
    "  - Posici√≥n del centro de la palma (x, y, z) (3 elementos)\n",
    "  - Posici√≥n de la manija de la puerta (x, y, z) (3 elementos)\n",
    "  - Diferencia posicional entre palma y manija (x, y, z) (3 elementos)\n",
    "  - Indicador de puerta abierta (1 elemento: 1 si abierta, -1 si cerrada)\n",
    "\n",
    "### Sistema de Recompensas (Versi√≥n Densa)\n",
    "\n",
    "La recompensa densa consiste en m√∫ltiples componentes:\n",
    "\n",
    "1. **get_to_handle:** Recompensa negativa creciente mientras m√°s lejos est√© la palma de la manija (escalada por 0.1)\n",
    "2. **open_door:** Error cuadr√°tico entre la posici√≥n actual de la bisagra y el estado de puerta abierta (escalada por 0.1)\n",
    "3. **velocity_penalty:** Penalizaci√≥n menor por velocidad para limitar la din√°mica del entorno (escalada por 0.00001)\n",
    "4. **door_hinge_displacement:** Recompensas positivas por progreso:\n",
    "   - +2 si la bisagra se abre m√°s de 0.2 radianes\n",
    "   - +8 si se abre m√°s de 1.0 radianes\n",
    "   - +10 si se abre m√°s de 1.35 radianes\n",
    "\n",
    "### Estados Finales\n",
    "\n",
    "- **Truncamiento:** Despu√©s de 200 pasos (configurable con max_episode_steps)\n",
    "- **Nunca termina:** La tarea es de horizonte infinito, se eval√∫a por progreso continuo\n",
    "- **√âxito:** Cuando la puerta toca el tope del otro lado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias requeridas\n",
    "!pip install gymnasium-robotics stable-baselines3[extra] tensorboard opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "import os\n",
    "\n",
    "# Stable Baselines3 imports\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Register gymnasium robotics environments\n",
    "gym.register_envs(gymnasium_robotics)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(\"‚úÖ Todas las dependencias importadas correctamente\")\n",
    "print(f\"üéØ Semilla aleatoria establecida: {SEED}\")\n",
    "print(f\"üñ•Ô∏è  Dispositivo PyTorch: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¬øPor qu√© SAC?\n",
    "\n",
    "Se escogi√≥ **Soft Actor-Critic (SAC)** como m√©todo principal para este proyecto debido a sus ventajas espec√≠ficas para tareas de manipulaci√≥n rob√≥tica con espacios de acci√≥n continuos y alta dimensionalidad.\n",
    "\n",
    "---\n",
    "\n",
    "## Ventajas de SAC para Adroit Hand Door\n",
    "\n",
    "- **Espacios de Acci√≥n Continuos:**\n",
    "  Las 28 articulaciones de la mano y brazo requieren control continuo preciso. SAC est√° espec√≠ficamente dise√±ado para manejar espacios de acci√≥n continuos de alta dimensionalidad.\n",
    "\n",
    "- **Maximizaci√≥n de Entrop√≠a:**\n",
    "  SAC maximiza tanto la recompensa como la entrop√≠a de la pol√≠tica, promoviendo exploraci√≥n natural. Esto es crucial para tareas de manipulaci√≥n donde hay m√∫ltiples formas de alcanzar el objetivo.\n",
    "\n",
    "- **Estabilidad en Off-Policy Learning:**\n",
    "  SAC es un algoritmo off-policy que puede reutilizar experiencias pasadas eficientemente, crucial para entornos donde la interacci√≥n es computacionalmente costosa.\n",
    "\n",
    "- **Robustez a Hiperpar√°metros:**\n",
    "  SAC es conocido por ser robusto a la selecci√≥n de hiperpar√°metros, reduciendo la necesidad de ajuste fino extensivo.\n",
    "\n",
    "- **Paralelizaci√≥n Eficiente:**\n",
    "  SAC se beneficia significativamente del entrenamiento paralelo con m√∫ltiples entornos, acelerando el aprendizaje.\n",
    "\n",
    "- **Manejo de Recompensas Densas:**\n",
    "  La estructura de recompensa densa de Adroit Hand Door se alinea bien con el aprendizaje continuo que SAC proporciona.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparaci√≥n con Otros Algoritmos\n",
    "\n",
    "| Algoritmo | Tipo | Ventajas para esta tarea | Desventajas |\n",
    "|-----------|------|--------------------------|-------------|\n",
    "| **SAC** | Off-policy, Actor-Critic | ‚úÖ Acciones continuas, exploraci√≥n autom√°tica, estable | Computacionalmente intensivo |\n",
    "| **PPO** | On-policy, Actor-Critic | ‚úÖ Estable, f√°cil de implementar | ‚ùå Menos eficiente en muestreo, exploraci√≥n limitada |\n",
    "| **TD3** | Off-policy, Actor-Critic | ‚úÖ Acciones continuas, eficiente | ‚ùå Requiere m√°s ajuste de hiperpar√°metros |\n",
    "| **DDPG** | Off-policy, Actor-Critic | ‚úÖ Acciones continuas | ‚ùå Menos estable, sensible a hiperpar√°metros |\n",
    "| **A3C** | On-policy, Actor-Critic | ‚úÖ Paralelizaci√≥n natural | ‚ùå On-policy, exploraci√≥n sub√≥ptima para manipulaci√≥n |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura del Sistema\n",
    "\n",
    "### Componentes Principales\n",
    "\n",
    "1. **AdroitHandDoorEnvironment**: Wrapper del entorno gymnasium-robotics\n",
    "2. **SB3SACAgent**: Agente SAC basado en Stable Baselines3\n",
    "3. **TrainingCallback**: Callback personalizado para evaluaci√≥n y guardado\n",
    "4. **Logger**: Sistema de logging y monitoreo de m√©tricas\n",
    "5. **Entrenamiento Paralelo**: Uso de m√∫ltiples entornos para acelerar el aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdroitHandDoorEnvironment:\n",
    "    \"\"\"Wrapper para el entorno Adroit Hand Door con funcionalidades adicionales\"\"\"\n",
    "    \n",
    "    def __init__(self, render_mode=None, seed=42, max_episode_steps=200, reward_type=\"dense\"):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno Adroit Hand Door\n",
    "        \n",
    "        Args:\n",
    "            render_mode: Modo de renderizado ('human', 'rgb_array', None)\n",
    "            seed: Semilla para reproducibilidad\n",
    "            max_episode_steps: M√°ximo n√∫mero de pasos por episodio\n",
    "            reward_type: Tipo de recompensa ('dense' o 'sparse')\n",
    "        \"\"\"\n",
    "        self.render_mode = render_mode\n",
    "        self.seed = seed\n",
    "        self.max_episode_steps = max_episode_steps\n",
    "        self.reward_type = reward_type\n",
    "        \n",
    "        # Crear el entorno base\n",
    "        env_id = f\"AdroitHandDoor-v1\"\n",
    "        self.env = gym.make(\n",
    "            env_id,\n",
    "            max_episode_steps=max_episode_steps,\n",
    "            render_mode=render_mode\n",
    "        )\n",
    "        \n",
    "        # Configurar semilla\n",
    "        if seed is not None:\n",
    "            self.env.reset(seed=seed)\n",
    "        \n",
    "        # Informaci√≥n del entorno\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "        \n",
    "        # M√©tricas de seguimiento\n",
    "        self.episode_steps = 0\n",
    "        self.episode_reward = 0\n",
    "        self.success_threshold = 0.95  # Umbral para considerar √©xito\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reinicia el entorno\"\"\"\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.episode_steps = 0\n",
    "        self.episode_reward = 0\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Ejecuta una acci√≥n en el entorno\"\"\"\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        self.episode_steps += 1\n",
    "        self.episode_reward += reward\n",
    "        \n",
    "        # Agregar informaci√≥n adicional\n",
    "        info['episode_steps'] = self.episode_steps\n",
    "        info['episode_reward'] = self.episode_reward\n",
    "        info['success'] = self.get_success_rate()\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "    def get_success_rate(self):\n",
    "        \"\"\"Calcula la tasa de √©xito basada en el estado de la puerta\"\"\"\n",
    "        try:\n",
    "            # En Adroit Hand Door, el √©xito se mide por qu√© tan abierta est√° la puerta\n",
    "            # La observaci√≥n 38 indica si la puerta est√° abierta (1) o cerrada (-1)\n",
    "            obs, _ = self.env.reset()\n",
    "            door_open = obs[38] if hasattr(obs, '__getitem__') and len(obs) > 38 else 0\n",
    "            return max(0, door_open)  # Normalizar a [0, 1]\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Renderiza el entorno\"\"\"\n",
    "        return self.env.render()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Cierra el entorno\"\"\"\n",
    "        self.env.close()\n",
    "    \n",
    "    def get_state_dim(self):\n",
    "        \"\"\"Retorna la dimensi√≥n del espacio de estados\"\"\"\n",
    "        return self.observation_space.shape[0]\n",
    "    \n",
    "    def get_action_dim(self):\n",
    "        \"\"\"Retorna la dimensi√≥n del espacio de acciones\"\"\"\n",
    "        return self.action_space.shape[0]\n",
    "    \n",
    "    def get_action_bounds(self):\n",
    "        \"\"\"Retorna los l√≠mites del espacio de acciones\"\"\"\n",
    "        return (self.action_space.low, self.action_space.high)\n",
    "\n",
    "# Crear una instancia de prueba para verificar las dimensiones\n",
    "test_env = AdroitHandDoorEnvironment(render_mode=None, seed=SEED)\n",
    "print(f\"‚úÖ Entorno Adroit Hand Door creado exitosamente\")\n",
    "print(f\"üìä Dimensiones del estado: {test_env.get_state_dim()}\")\n",
    "print(f\"üéÆ Dimensiones de la acci√≥n: {test_env.get_action_dim()}\")\n",
    "print(f\"üìè L√≠mites de acci√≥n: {test_env.get_action_bounds()[0][:5]}... a {test_env.get_action_bounds()[1][:5]}...\")\n",
    "print(f\"üéØ Espacio de observaci√≥n: {test_env.observation_space}\")\n",
    "print(f\"üïπÔ∏è  Espacio de acci√≥n: {test_env.action_space}\")\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, el entorno Adroit Hand Door tiene:\n",
    "- **39 dimensiones de estado**: Incluyendo posiciones de articulaciones, estado de la puerta, y posiciones relativas\n",
    "- **28 dimensiones de acci√≥n**: Correspondientes a las 28 articulaciones de la mano y brazo\n",
    "- **Espacio de acci√≥n continuo**: Valores entre -1 y 1 para cada articulaci√≥n\n",
    "\n",
    "Esta alta dimensionalidad hace que SAC sea una elecci√≥n ideal debido a su capacidad para manejar espacios de acci√≥n continuos complejos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    \"\"\"Sistema de logging para m√©tricas de entrenamiento\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir='logs/sac_parallel', tensorboard_dir='runs/sac_parallel'):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.tensorboard_dir = Path(tensorboard_dir)\n",
    "        \n",
    "        # Crear directorios\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.tensorboard_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Historia de m√©tricas\n",
    "        self.history = {\n",
    "            'timesteps': [],\n",
    "            'episodes': [],\n",
    "            'rewards': [],\n",
    "            'episode_lengths': [],\n",
    "            'success_rates': [],\n",
    "            'eval_rewards': [],\n",
    "            'eval_success_rates': [],\n",
    "            'timestamps': []\n",
    "        }\n",
    "        \n",
    "        # Inicializar CSV\n",
    "        self.csv_path = self.log_dir / 'training_log.csv'\n",
    "        self._init_csv()\n",
    "    \n",
    "    def _init_csv(self):\n",
    "        \"\"\"Inicializa el archivo CSV con headers\"\"\"\n",
    "        with open(self.csv_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\n",
    "                'timestep', 'episode', 'reward', 'episode_length', \n",
    "                'success_rate', 'eval_reward', 'eval_success_rate', 'timestamp'\n",
    "            ])\n",
    "    \n",
    "    def log_episode(self, timestep, episode, reward, episode_length, success_rate=None, \n",
    "                   eval_reward=None, eval_success_rate=None):\n",
    "        \"\"\"Registra m√©tricas de un episodio\"\"\"\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        # Agregar a historia\n",
    "        self.history['timesteps'].append(timestep)\n",
    "        self.history['episodes'].append(episode)\n",
    "        self.history['rewards'].append(reward)\n",
    "        self.history['episode_lengths'].append(episode_length)\n",
    "        self.history['success_rates'].append(success_rate or 0.0)\n",
    "        self.history['eval_rewards'].append(eval_reward or 0.0)\n",
    "        self.history['eval_success_rates'].append(eval_success_rate or 0.0)\n",
    "        self.history['timestamps'].append(timestamp)\n",
    "        \n",
    "        # Escribir a CSV\n",
    "        with open(self.csv_path, 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\n",
    "                timestep, episode, reward, episode_length,\n",
    "                success_rate or 0.0, eval_reward or 0.0, \n",
    "                eval_success_rate or 0.0, timestamp\n",
    "            ])\n",
    "    \n",
    "    def get_recent_stats(self, n=100):\n",
    "        \"\"\"Obtiene estad√≠sticas de los √∫ltimos n episodios\"\"\"\n",
    "        if len(self.history['rewards']) < n:\n",
    "            n = len(self.history['rewards'])\n",
    "        \n",
    "        if n == 0:\n",
    "            return {}\n",
    "        \n",
    "        recent_rewards = self.history['rewards'][-n:]\n",
    "        recent_success = self.history['success_rates'][-n:]\n",
    "        \n",
    "        return {\n",
    "            'mean_reward': np.mean(recent_rewards),\n",
    "            'std_reward': np.std(recent_rewards),\n",
    "            'mean_success': np.mean(recent_success),\n",
    "            'episodes_logged': len(self.history['episodes'])\n",
    "        }\n",
    "    \n",
    "    def save_history(self):\n",
    "        \"\"\"Guarda la historia completa\"\"\"\n",
    "        history_path = self.log_dir / 'training_history.json'\n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(self.history, f, indent=2)\n",
    "        \n",
    "        print(f\"üìä Historia guardada en: {history_path}\")\n",
    "        print(f\"üìà M√©tricas CSV en: {self.csv_path}\")\n",
    "\n",
    "print(\"‚úÖ Logger implementado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingCallback(BaseCallback):\n",
    "    \"\"\"Callback personalizado para evaluaci√≥n y guardado durante el entrenamiento\"\"\"\n",
    "    \n",
    "    def __init__(self, eval_freq, save_freq, eval_episodes=5, save_path='models/sac_parallel', \n",
    "                 verbose=1, custom_logger=None):\n",
    "        super(TrainingCallback, self).__init__(verbose)\n",
    "        self.eval_freq = eval_freq\n",
    "        self.save_freq = save_freq\n",
    "        self.eval_episodes = eval_episodes\n",
    "        self.save_path = Path(save_path)\n",
    "        self.save_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.custom_logger = custom_logger\n",
    "        \n",
    "        # M√©tricas de seguimiento\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.episode_count = 0\n",
    "        self.last_eval_timestep = 0\n",
    "        self.last_save_timestep = 0\n",
    "        \n",
    "        # Crear entorno de evaluaci√≥n\n",
    "        self.eval_env = None\n",
    "    \n",
    "    def _init_callback(self) -> None:\n",
    "        \"\"\"Inicializa el callback\"\"\"\n",
    "        # Crear entorno de evaluaci√≥n\n",
    "        if self.eval_env is None:\n",
    "            self.eval_env = AdroitHandDoorEnvironment(\n",
    "                render_mode=None,\n",
    "                seed=SEED + 1000,  # Semilla diferente para evaluaci√≥n\n",
    "                max_episode_steps=200,\n",
    "                reward_type=\"dense\"\n",
    "            )\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"Ejecutado en cada paso de entrenamiento\"\"\"\n",
    "        \n",
    "        # Evaluaci√≥n peri√≥dica\n",
    "        if (self.num_timesteps - self.last_eval_timestep) >= self.eval_freq:\n",
    "            self._evaluate_model()\n",
    "            self.last_eval_timestep = self.num_timesteps\n",
    "        \n",
    "        # Guardado peri√≥dico\n",
    "        if (self.num_timesteps - self.last_save_timestep) >= self.save_freq:\n",
    "            self._save_model()\n",
    "            self.last_save_timestep = self.num_timesteps\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _evaluate_model(self):\n",
    "        \"\"\"Eval√∫a el modelo actual\"\"\"\n",
    "        if self.eval_env is None:\n",
    "            return\n",
    "        \n",
    "        eval_rewards = []\n",
    "        eval_success_rates = []\n",
    "        \n",
    "        for _ in range(self.eval_episodes):\n",
    "            obs, _ = self.eval_env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "            \n",
    "            while not done and episode_steps < 200:\n",
    "                action, _ = self.model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = self.eval_env.step(action)\n",
    "                episode_reward += reward\n",
    "                episode_steps += 1\n",
    "                done = terminated or truncated\n",
    "            \n",
    "            eval_rewards.append(episode_reward)\n",
    "            eval_success_rates.append(self.eval_env.get_success_rate())\n",
    "        \n",
    "        mean_reward = np.mean(eval_rewards)\n",
    "        mean_success = np.mean(eval_success_rates)\n",
    "        \n",
    "        # Log de evaluaci√≥n\n",
    "        if self.verbose > 0:\n",
    "            print(f\"\\nüîç Evaluaci√≥n en timestep {self.num_timesteps:,}:\")\n",
    "            print(f\"   Recompensa media: {mean_reward:.2f} ¬± {np.std(eval_rewards):.2f}\")\n",
    "            print(f\"   Tasa de √©xito media: {mean_success:.3f}\")\n",
    "        \n",
    "        # Guardar mejor modelo\n",
    "        if mean_reward > self.best_mean_reward:\n",
    "            self.best_mean_reward = mean_reward\n",
    "            best_model_path = self.save_path / \"sac_best.zip\"\n",
    "            self.model.save(best_model_path)\n",
    "            if self.verbose > 0:\n",
    "                print(f\"   üíæ ¬°Nuevo mejor modelo guardado! Recompensa: {mean_reward:.2f}\")\n",
    "        \n",
    "        # Log personalizado\n",
    "        if self.custom_logger:\n",
    "            self.custom_logger.log_episode(\n",
    "                timestep=self.num_timesteps,\n",
    "                episode=self.episode_count,\n",
    "                reward=mean_reward,\n",
    "                episode_length=episode_steps,\n",
    "                success_rate=mean_success,\n",
    "                eval_reward=mean_reward,\n",
    "                eval_success_rate=mean_success\n",
    "            )\n",
    "        \n",
    "        self.episode_count += 1\n",
    "    \n",
    "    def _save_model(self):\n",
    "        \"\"\"Guarda el modelo actual\"\"\"\n",
    "        checkpoint_path = self.save_path / f\"sac_checkpoint_{self.num_timesteps}.zip\"\n",
    "        self.model.save(checkpoint_path)\n",
    "        if self.verbose > 0:\n",
    "            print(f\"üíæ Checkpoint guardado en timestep {self.num_timesteps:,}\")\n",
    "\n",
    "print(\"‚úÖ TrainingCallback implementado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SB3SACAgent:\n",
    "    \"\"\"Agente SAC usando Stable Baselines3 con entrenamiento paralelo\"\"\"\n",
    "    \n",
    "    def __init__(self, env, model_dir='models/sac_parallel', **sac_params):\n",
    "        self.env_factory = env\n",
    "        self.model_dir = Path(model_dir)\n",
    "        self.model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Extraer par√°metros espec√≠ficos de SAC\n",
    "        self.n_envs = sac_params.pop('n_envs', 4)\n",
    "        self.vec_env_cls = sac_params.pop('vec_env_cls', SubprocVecEnv)\n",
    "        \n",
    "        # Crear entornos vectorizados\n",
    "        self.env = make_vec_env(\n",
    "            env_id=self._make_env_wrapper,\n",
    "            n_envs=self.n_envs,\n",
    "            vec_env_cls=self.vec_env_cls,\n",
    "            seed=SEED\n",
    "        )\n",
    "        \n",
    "        # Crear modelo SAC\n",
    "        self.model = SAC(\n",
    "            policy=\"MlpPolicy\",\n",
    "            env=self.env,\n",
    "            **sac_params\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Agente SAC creado con {self.n_envs} entornos paralelos\")\n",
    "        print(f\"üîß Vectorizaci√≥n: {self.vec_env_cls.__name__}\")\n",
    "        print(f\"üñ•Ô∏è  Dispositivo: {self.model.device}\")\n",
    "    \n",
    "    def _make_env_wrapper(self):\n",
    "        \"\"\"Wrapper para crear entornos individuales\"\"\"\n",
    "        return self.env_factory()\n",
    "    \n",
    "    def train(self, total_timesteps, callback=None, progress_bar=True):\n",
    "        \"\"\"Entrena el agente SAC\"\"\"\n",
    "        print(f\"üöÄ Iniciando entrenamiento por {total_timesteps:,} timesteps...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.model.learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            callback=callback,\n",
    "            progress_bar=progress_bar\n",
    "        )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"‚úÖ Entrenamiento completado en {training_time/3600:.2f} horas\")\n",
    "        print(f\"‚ö° Velocidad: {total_timesteps/(training_time/3600):,.0f} timesteps/hora\")\n",
    "    \n",
    "    def save(self, filename=None):\n",
    "        \"\"\"Guarda el modelo\"\"\"\n",
    "        if filename is None:\n",
    "            filename = \"sac_final.zip\"\n",
    "        \n",
    "        filepath = self.model_dir / filename\n",
    "        self.model.save(filepath)\n",
    "        print(f\"üíæ Modelo guardado en: {filepath}\")\n",
    "    \n",
    "    def load(self, filename=None):\n",
    "        \"\"\"Carga un modelo existente\"\"\"\n",
    "        if filename is None:\n",
    "            # Buscar el mejor modelo disponible\n",
    "            best_path = self.model_dir / \"sac_best.zip\"\n",
    "            final_path = self.model_dir / \"sac_final.zip\"\n",
    "            \n",
    "            if best_path.exists():\n",
    "                filepath = best_path\n",
    "            elif final_path.exists():\n",
    "                filepath = final_path\n",
    "            else:\n",
    "                print(\"‚ùå No se encontraron modelos guardados\")\n",
    "                return False\n",
    "        else:\n",
    "            filepath = self.model_dir / filename\n",
    "        \n",
    "        if not filepath.exists():\n",
    "            print(f\"‚ùå Modelo no encontrado: {filepath}\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            self.model = SAC.load(filepath, env=self.env)\n",
    "            print(f\"‚úÖ Modelo cargado desde: {filepath}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error cargando modelo: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def predict(self, observation, deterministic=True):\n",
    "        \"\"\"Predice una acci√≥n para una observaci√≥n dada\"\"\"\n",
    "        return self.model.predict(observation, deterministic=deterministic)\n",
    "    \n",
    "    def evaluate(self, n_episodes=10, render=False):\n",
    "        \"\"\"Eval√∫a el agente entrenado\"\"\"\n",
    "        # Crear entorno de evaluaci√≥n\n",
    "        eval_env = AdroitHandDoorEnvironment(\n",
    "            render_mode=\"human\" if render else None,\n",
    "            seed=SEED + 2000,\n",
    "            max_episode_steps=200,\n",
    "            reward_type=\"dense\"\n",
    "        )\n",
    "        \n",
    "        episode_rewards = []\n",
    "        episode_successes = []\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            obs, _ = eval_env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done and episode_steps < 200:\n",
    "                action, _ = self.model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "                episode_reward += reward\n",
    "                episode_steps += 1\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                if render:\n",
    "                    eval_env.render()\n",
    "                    time.sleep(0.01)  # Peque√±a pausa para visualizaci√≥n\n",
    "            \n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_successes.append(eval_env.get_success_rate())\n",
    "            \n",
    "            print(f\"Episodio {episode + 1:2d}: Recompensa = {episode_reward:7.1f}, \"\n",
    "                  f\"√âxito = {eval_env.get_success_rate():.3f}, Pasos = {episode_steps}\")\n",
    "        \n",
    "        eval_env.close()\n",
    "        \n",
    "        # Calcular estad√≠sticas\n",
    "        mean_reward = np.mean(episode_rewards)\n",
    "        std_reward = np.std(episode_rewards)\n",
    "        mean_success = np.mean(episode_successes)\n",
    "        \n",
    "        print(f\"\\nüìä Resultados de Evaluaci√≥n ({n_episodes} episodios):\")\n",
    "        print(f\"   Recompensa promedio: {mean_reward:.2f} ¬± {std_reward:.2f}\")\n",
    "        print(f\"   Tasa de √©xito promedio: {mean_success:.3f}\")\n",
    "        print(f\"   Recompensa m√°xima: {max(episode_rewards):.2f}\")\n",
    "        print(f\"   Recompensa m√≠nima: {min(episode_rewards):.2f}\")\n",
    "        \n",
    "        return {\n",
    "            'mean_reward': mean_reward,\n",
    "            'std_reward': std_reward,\n",
    "            'mean_success': mean_success,\n",
    "            'all_rewards': episode_rewards,\n",
    "            'all_successes': episode_successes\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ SB3SACAgent implementado correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuraci√≥n de Entrenamiento\n",
    "\n",
    "## Hiperpar√°metros Optimizados para SAC\n",
    "\n",
    "Los hiperpar√°metros han sido cuidadosamente seleccionados para la tarea de manipulaci√≥n rob√≥tica:\n",
    "\n",
    "### Par√°metros de Red\n",
    "- **Arquitectura de red:** 512x512 para actor y cr√≠tico\n",
    "- **Funci√≥n de activaci√≥n:** ReLU\n",
    "- **Learning rate:** 1e-4 (conservador para estabilidad)\n",
    "\n",
    "### Par√°metros de Aprendizaje\n",
    "- **Tama√±o de buffer:** 1,000,000 (grande para retener experiencias diversas)\n",
    "- **Batch size:** 256 (balance entre estabilidad y eficiencia)\n",
    "- **Gamma (descuento):** 0.99 (horizonte largo para manipulaci√≥n)\n",
    "- **Tau (soft update):** 0.005 (actualizaciones suaves)\n",
    "\n",
    "### Par√°metros de Exploraci√≥n\n",
    "- **Coeficiente de entrop√≠a:** Auto (SAC ajusta autom√°ticamente)\n",
    "- **Target entropy:** Auto (basado en dimensi√≥n de acci√≥n)\n",
    "\n",
    "### Entrenamiento Paralelo\n",
    "- **N√∫mero de entornos:** 4 (balance entre paralelizaci√≥n y recursos)\n",
    "- **Vectorizaci√≥n:** SubprocVecEnv (verdadero paralelismo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sac_parallel(\n",
    "    episodes=5000,\n",
    "    n_envs=4,\n",
    "    eval_interval=2000,  # Timesteps entre evaluaciones\n",
    "    save_interval=50000,  # Timesteps entre guardados\n",
    "    render=False,\n",
    "    restart=False,\n",
    "    reward_type='dense',\n",
    "    max_episode_steps=200,\n",
    "    vec_env_cls=SubprocVecEnv\n",
    "):\n",
    "    \"\"\"\n",
    "    Entrena un agente SAC con entornos paralelos en Adroit Hand Door.\n",
    "    \n",
    "    Args:\n",
    "        episodes: N√∫mero de episodios de entrenamiento por entorno\n",
    "        n_envs: N√∫mero de entornos paralelos\n",
    "        eval_interval: Timesteps entre evaluaciones\n",
    "        save_interval: Timesteps entre checkpoints\n",
    "        render: Habilitar renderizado durante entrenamiento (solo n_envs=1)\n",
    "        restart: Cargar y continuar desde el mejor modelo guardado\n",
    "        reward_type: Tipo de funci√≥n de recompensa ('dense' o 'sparse')\n",
    "        max_episode_steps: M√°ximo de pasos por episodio\n",
    "        vec_env_cls: Clase de entorno vectorizado\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üöÄ INICIANDO ENTRENAMIENTO SAC PARALELO EN ADROIT HAND DOOR\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Deshabilitar renderizado para entrenamiento paralelo\n",
    "    if n_envs > 1 and render:\n",
    "        print(\"‚ö†Ô∏è  Advertencia: Renderizado deshabilitado para entrenamiento paralelo (n_envs > 1)\")\n",
    "        render = False\n",
    "    \n",
    "    # Factory function para crear entornos\n",
    "    def env_factory():\n",
    "        return AdroitHandDoorEnvironment(\n",
    "            render_mode=\"human\" if render and n_envs == 1 else None,\n",
    "            seed=SEED,\n",
    "            max_episode_steps=max_episode_steps,\n",
    "            reward_type=reward_type\n",
    "        )\n",
    "    \n",
    "    # Obtener informaci√≥n del entorno desde una instancia √∫nica\n",
    "    single_env = env_factory()\n",
    "    state_dim = single_env.get_state_dim()\n",
    "    action_dim = single_env.get_action_dim()\n",
    "    action_bounds = single_env.get_action_bounds()\n",
    "    single_env.close()\n",
    "    \n",
    "    print(f\"üìä Informaci√≥n del entorno:\")\n",
    "    print(f\"   Dimensi√≥n del estado: {state_dim}\")\n",
    "    print(f\"   Dimensi√≥n de la acci√≥n: {action_dim}\")\n",
    "    print(f\"   L√≠mites de acci√≥n: [{action_bounds[0][0]:.1f}, {action_bounds[1][0]:.1f}]\")\n",
    "    print(f\"   Entornos paralelos: {n_envs}\")\n",
    "    print(f\"   Vectorizaci√≥n: {vec_env_cls.__name__}\")\n",
    "    \n",
    "    # Hiperpar√°metros SAC optimizados para entrenamiento paralelo\n",
    "    sac_params = {\n",
    "        'learning_rate': 1e-4,\n",
    "        'buffer_size': 1_000_000,\n",
    "        'learning_starts': 10000,\n",
    "        'batch_size': 256,\n",
    "        'tau': 0.005,\n",
    "        'gamma': 0.99,\n",
    "        'train_freq': 1,\n",
    "        'gradient_steps': 1,  # Mantener gradiente constante para estabilidad\n",
    "        'ent_coef': 'auto',\n",
    "        'target_update_interval': 1,\n",
    "        'target_entropy': 'auto',\n",
    "        'use_sde': False,\n",
    "        'policy_kwargs': dict(\n",
    "            net_arch=dict(pi=[512, 512], qf=[512, 512]),\n",
    "            activation_fn=torch.nn.ReLU\n",
    "        ),\n",
    "        'verbose': 1,\n",
    "        'seed': SEED,\n",
    "        'tensorboard_log': 'runs/sac_parallel',\n",
    "        'n_envs': n_envs,\n",
    "        'vec_env_cls': vec_env_cls\n",
    "    }\n",
    "    \n",
    "    # Inicializar agente SAC con entornos paralelos\n",
    "    agent = SB3SACAgent(\n",
    "        env=env_factory,\n",
    "        model_dir='models/sac_parallel',\n",
    "        **sac_params\n",
    "    )\n",
    "    \n",
    "    # Cargar modelos existentes si se especifica restart\n",
    "    if restart:\n",
    "        print(\"üîÑ Cargando modelos existentes...\")\n",
    "        if agent.load():\n",
    "            print(\"‚úÖ Modelos cargados exitosamente\")\n",
    "        else:\n",
    "            print(\"‚ùå No se encontraron modelos, iniciando desde cero\")\n",
    "    \n",
    "    print(f\"üñ•Ô∏è  Dispositivo: {agent.model.device}\")\n",
    "    print(f\"üì¶ Tama√±o del buffer: {agent.model.buffer_size:,}\")\n",
    "    print(f\"üìä Tama√±o del batch: {agent.model.batch_size}\")\n",
    "    print(f\"‚ö° Pasos de gradiente por actualizaci√≥n: {agent.model.gradient_steps}\")\n",
    "    \n",
    "    # Calcular timesteps totales\n",
    "    total_timesteps = episodes * max_episode_steps * n_envs\n",
    "    print(f\"üéØ Timesteps totales a entrenar: {total_timesteps:,}\")\n",
    "    print(f\"üìà Evaluaci√≥n cada: {eval_interval:,} timesteps\")\n",
    "    print(f\"üíæ Guardar checkpoint cada: {save_interval:,} timesteps\")\n",
    "    print(f\"üéÅ Tipo de recompensa: {reward_type}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Inicializar logger para logging CSV\n",
    "    logger = Logger(log_dir='logs/sac_parallel', tensorboard_dir='runs/sac_parallel')\n",
    "    \n",
    "    # Configurar callback de entrenamiento\n",
    "    callback = TrainingCallback(\n",
    "        eval_freq=eval_interval,\n",
    "        save_freq=save_interval,\n",
    "        eval_episodes=5,\n",
    "        save_path='models/sac_parallel',\n",
    "        verbose=1,\n",
    "        custom_logger=logger\n",
    "    )\n",
    "    \n",
    "    print(f\"üöÄ Iniciando entrenamiento paralelo...\")\n",
    "    print(f\"üìä Episodios esperados por entorno: ~{episodes}\")\n",
    "    print(f\"üìà Total de episodios esperados en todos los entornos: ~{episodes * n_envs}\")\n",
    "    print(f\"‚è±Ô∏è  Entrenando por {total_timesteps:,} timesteps totales\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Entrenar el agente\n",
    "    agent.train(\n",
    "        total_timesteps=total_timesteps,\n",
    "        callback=callback,\n",
    "        progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Guardado final\n",
    "    print(\"üíæ Guardando modelos finales...\")\n",
    "    agent.save()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üéâ ¬°ENTRENAMIENTO PARALELO COMPLETADO!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üèÉ Entornos de entrenamiento: {n_envs}\")\n",
    "    print(f\"üîß Vectorizaci√≥n: {vec_env_cls.__name__}\")\n",
    "    print(f\"‚è±Ô∏è  Tiempo total de entrenamiento: {total_time/3600:.2f} horas\")\n",
    "    print(f\"üìä Timesteps totales: {total_timesteps:,}\")\n",
    "    print(f\"‚ö° Timesteps por hora: {total_timesteps/(total_time/3600):,.0f}\")\n",
    "    print(f\"üöÄ Aceleraci√≥n vs. entorno √∫nico: ~{n_envs}x (te√≥rica)\")\n",
    "    \n",
    "    # Guardar informaci√≥n del entrenamiento\n",
    "    training_info = {\n",
    "        'n_envs': n_envs,\n",
    "        'vec_env_cls': vec_env_cls.__name__,\n",
    "        'total_timesteps': total_timesteps,\n",
    "        'training_time_hours': total_time / 3600,\n",
    "        'timesteps_per_hour': total_timesteps / (total_time / 3600),\n",
    "        'theoretical_speedup': n_envs,\n",
    "        'sac_params': sac_params,\n",
    "        'reward_type': reward_type,\n",
    "        'max_episode_steps': max_episode_steps,\n",
    "        'completion_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    info_path = Path('models/sac_parallel/training_info.json')\n",
    "    info_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(info_path, 'w') as f:\n",
    "        json.dump(training_info, f, indent=2)\n",
    "    \n",
    "    # Guardar historia del logger\n",
    "    logger.save_history()\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Limpiar recursos\n",
    "    agent.env.close()\n",
    "    \n",
    "    return agent\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de entrenamiento definida correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del Agente\n",
    "\n",
    "## Par√°metros de Entrenamiento\n",
    "\n",
    "- **Episodios:** 5000 por entorno (20,000 episodios totales con 4 entornos)\n",
    "- **Timesteps totales:** ~4,000,000 (5000 √ó 200 √ó 4)\n",
    "- **Entornos paralelos:** 4 (para acelerar el entrenamiento)\n",
    "- **Evaluaci√≥n:** Cada 2000 timesteps\n",
    "- **Guardado:** Cada 50,000 timesteps\n",
    "\n",
    "### Estimaci√≥n de Tiempo\n",
    "Con 4 entornos paralelos, esperamos una aceleraci√≥n de ~3-4x comparado con entrenamiento secuencial. El entrenamiento completo deber√≠a tomar aproximadamente 8-12 horas dependiendo del hardware.\n",
    "\n",
    "**Nota:** Para este notebook de demostraci√≥n, usaremos menos episodios. Para entrenamiento completo, aumentar a 5000+ episodios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar entrenamiento con par√°metros reducidos para demostraci√≥n\n",
    "# Para entrenamiento completo, usar episodes=5000 o m√°s\n",
    "\n",
    "trained_agent = train_sac_parallel(\n",
    "    episodes=2000,  # Reducido para demostraci√≥n - usar 5000+ para entrenamiento completo\n",
    "    n_envs=4,       # 4 entornos paralelos\n",
    "    eval_interval=5000,    # Evaluar cada 5000 timesteps\n",
    "    save_interval=20000,   # Guardar cada 20000 timesteps\n",
    "    render=False,   # Sin renderizado durante entrenamiento\n",
    "    restart=False,  # Iniciar desde cero (cambiar a True para continuar entrenamiento)\n",
    "    reward_type='dense',\n",
    "    max_episode_steps=200,\n",
    "    vec_env_cls=SubprocVecEnv\n",
    ")\n",
    "\n",
    "print(\"\\nüéâ ¬°Entrenamiento completado exitosamente!\")\n",
    "print(\"üìÅ Modelos guardados en: models/sac_parallel/\")\n",
    "print(\"üìä Logs guardados en: logs/sac_parallel/\")\n",
    "print(\"üìà TensorBoard logs en: runs/sac_parallel/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluaci√≥n del Agente Entrenado\n",
    "\n",
    "Ahora evaluaremos el rendimiento del agente entrenado ejecutando m√∫ltiples episodios y analizando las m√©tricas de rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trained_agent(model_path='models/sac_parallel/sac_best.zip', \n",
    "                          episodes=10, render=False):\n",
    "    \"\"\"Eval√∫a un agente SAC entrenado\"\"\"\n",
    "    print(\"üîç Evaluando agente entrenado...\")\n",
    "    \n",
    "    # Crear entorno de evaluaci√≥n\n",
    "    eval_env = AdroitHandDoorEnvironment(\n",
    "        render_mode=\"human\" if render else None,\n",
    "        seed=SEED + 3000,  # Semilla diferente para evaluaci√≥n\n",
    "        max_episode_steps=200,\n",
    "        reward_type=\"dense\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Cargar el modelo entrenado\n",
    "        model = SAC.load(model_path, env=eval_env.env)\n",
    "        \n",
    "        total_rewards = []\n",
    "        total_successes = []\n",
    "        episode_lengths = []\n",
    "        \n",
    "        print(f\"\\nüìä Ejecutando {episodes} episodios de evaluaci√≥n...\")\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            obs, _ = eval_env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done and episode_steps < 200:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "                episode_reward += reward\n",
    "                episode_steps += 1\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                if render:\n",
    "                    eval_env.render()\n",
    "                    time.sleep(0.01)\n",
    "            \n",
    "            success_rate = eval_env.get_success_rate()\n",
    "            \n",
    "            total_rewards.append(episode_reward)\n",
    "            total_successes.append(success_rate)\n",
    "            episode_lengths.append(episode_steps)\n",
    "            \n",
    "            print(f\"Episodio {i+1:2d}: Recompensa = {episode_reward:7.1f}, \"\n",
    "                  f\"√âxito = {success_rate:.3f}, Pasos = {episode_steps}\")\n",
    "        \n",
    "        # Calcular estad√≠sticas\n",
    "        avg_reward = np.mean(total_rewards)\n",
    "        std_reward = np.std(total_rewards)\n",
    "        avg_success = np.mean(total_successes)\n",
    "        avg_length = np.mean(episode_lengths)\n",
    "        \n",
    "        # Calcular tasa de √©xito binaria (episodios con success > 0.5)\n",
    "        success_rate_binary = np.sum(np.array(total_successes) > 0.5) / episodes * 100\n",
    "        \n",
    "        print(f\"\\nüìä Resultados de Evaluaci√≥n ({episodes} episodios):\")\n",
    "        print(f\"   Recompensa promedio: {avg_reward:.2f} ¬± {std_reward:.2f}\")\n",
    "        print(f\"   Recompensa m√°xima: {max(total_rewards):.2f}\")\n",
    "        print(f\"   Recompensa m√≠nima: {min(total_rewards):.2f}\")\n",
    "        print(f\"   √âxito promedio: {avg_success:.3f}\")\n",
    "        print(f\"   Tasa de √©xito (>0.5): {success_rate_binary:.1f}%\")\n",
    "        print(f\"   Duraci√≥n promedio: {avg_length:.1f} pasos\")\n",
    "        \n",
    "        return {\n",
    "            'rewards': total_rewards,\n",
    "            'successes': total_successes,\n",
    "            'lengths': episode_lengths,\n",
    "            'avg_reward': avg_reward,\n",
    "            'std_reward': std_reward,\n",
    "            'avg_success': avg_success,\n",
    "            'success_rate_binary': success_rate_binary,\n",
    "            'avg_length': avg_length\n",
    "        }\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Modelo no encontrado en: {model_path}\")\n",
    "        print(\"üí° Aseg√∫rate de haber ejecutado el entrenamiento primero.\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        eval_env.close()\n",
    "\n",
    "# Evaluar el agente entrenado\n",
    "eval_results = evaluate_trained_agent(\n",
    "    episodes=20,  # Aumentar para evaluaci√≥n m√°s completa\n",
    "    render=False  # Cambiar a True para visualizar el agente\n",
    ")\n",
    "\n",
    "if eval_results:\n",
    "    print(\"\\n‚úÖ Evaluaci√≥n completada exitosamente\")\n",
    "else:\n",
    "    print(\"‚ùå Evaluaci√≥n fall√≥ - verifica que el modelo est√© entrenado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lisis de Resultados\n",
    "\n",
    "Para este an√°lisis de resultados de Adroit Hand Door utilizaremos las siguientes m√©tricas clave:\n",
    "\n",
    "1. **Recompensa por timestep** (tendencia de aprendizaje)\n",
    "2. **Tasa de √©xito** (qu√© tan bien abre la puerta)\n",
    "3. **Duraci√≥n de episodios** (eficiencia del agente)\n",
    "4. **Recompensas de evaluaci√≥n** (rendimiento en episodios deterministas)\n",
    "\n",
    "## ¬øQu√© nos dice cada m√©trica?\n",
    "\n",
    "### Recompensa por Timestep\n",
    "En tareas de manipulaci√≥n rob√≥tica como Adroit Hand Door, la recompensa por timestep indica:\n",
    "- **Tendencia creciente:** El agente aprende a acercarse y manipular la manija m√°s eficientemente\n",
    "- **Estabilizaci√≥n:** El agente ha convergido a una pol√≠tica consistente\n",
    "- **Variabilidad:** Natural en tareas complejas debido a aleatoriedad en posici√≥n de la puerta\n",
    "\n",
    "### Tasa de √âxito\n",
    "La m√©trica m√°s importante para evaluar el rendimiento:\n",
    "- **Valores cercanos a 1.0:** Agente logra abrir la puerta consistentemente\n",
    "- **Mejora gradual:** Indica aprendizaje progresivo de la secuencia de manipulaci√≥n\n",
    "- **Plateau alto:** Convergencia exitosa de la pol√≠tica\n",
    "\n",
    "### Duraci√≥n de Episodios\n",
    "- **Episodios m√°s largos inicialmente:** Exploraci√≥n y aprendizaje\n",
    "- **Estabilizaci√≥n:** Agente desarrolla estrategia eficiente\n",
    "- **Consistencia:** Indica pol√≠tica robusta y determinista\n",
    "\n",
    "### Recompensas de Evaluaci√≥n\n",
    "- **Evaluaci√≥n determinista:** Sin exploraci√≥n, solo explotaci√≥n de la pol√≠tica aprendida\n",
    "- **Consistencia alta:** Indica aprendizaje robusto\n",
    "- **Mejora continua:** Pol√≠tica sigue optimiz√°ndose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y analizar datos de entrenamiento\n",
    "try:\n",
    "    # Cargar datos del CSV de entrenamiento\n",
    "    training_data = pd.read_csv('logs/sac_parallel/training_log.csv')\n",
    "    print(f\"üìä Datos de entrenamiento cargados: {len(training_data)} registros\")\n",
    "    \n",
    "    # Verificar columnas disponibles\n",
    "    print(f\"üìà Columnas disponibles: {list(training_data.columns)}\")\n",
    "    \n",
    "    # Calcular promedios m√≥viles para suavizar las curvas\n",
    "    window_size = min(50, len(training_data) // 10)  # Ventana adaptativa\n",
    "    \n",
    "    if window_size > 1:\n",
    "        training_data['reward_ma'] = training_data['reward'].rolling(window=window_size, min_periods=1).mean()\n",
    "        training_data['success_ma'] = training_data['success_rate'].rolling(window=window_size, min_periods=1).mean()\n",
    "        training_data['eval_reward_ma'] = training_data['eval_reward'].rolling(window=window_size, min_periods=1).mean()\n",
    "    \n",
    "    # Crear visualizaci√≥n completa de m√©tricas de entrenamiento\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('M√©tricas de Entrenamiento SAC - Adroit Hand Door', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Recompensas de Entrenamiento\n",
    "    axes[0,0].plot(training_data['timestep'], training_data['reward'], alpha=0.3, color='lightblue', label='Recompensa por Episodio')\n",
    "    if window_size > 1:\n",
    "        axes[0,0].plot(training_data['timestep'], training_data['reward_ma'], color='blue', linewidth=2, label=f'Promedio M√≥vil ({window_size})')\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.7, label='L√≠nea Base (0)')\n",
    "    axes[0,0].set_xlabel('Timesteps')\n",
    "    axes[0,0].set_ylabel('Recompensa')\n",
    "    axes[0,0].set_title('Recompensas de Entrenamiento a lo Largo del Tiempo')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Tasa de √âxito\n",
    "    axes[0,1].plot(training_data['timestep'], training_data['success_rate'], alpha=0.4, color='green', label='Tasa de √âxito')\n",
    "    if window_size > 1:\n",
    "        axes[0,1].plot(training_data['timestep'], training_data['success_ma'], color='darkgreen', linewidth=2, label=f'Promedio M√≥vil ({window_size})')\n",
    "    axes[0,1].axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='Umbral de √âxito (0.5)')\n",
    "    axes[0,1].axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Buen Rendimiento (0.8)')\n",
    "    axes[0,1].set_xlabel('Timesteps')\n",
    "    axes[0,1].set_ylabel('Tasa de √âxito')\n",
    "    axes[0,1].set_title('Tasa de √âxito Durante el Entrenamiento')\n",
    "    axes[0,1].set_ylim(0, 1)\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Duraci√≥n de Episodios\n",
    "    axes[1,0].plot(training_data['timestep'], training_data['episode_length'], alpha=0.4, color='purple', label='Pasos por Episodio')\n",
    "    axes[1,0].axhline(y=200, color='red', linestyle='--', alpha=0.7, label='M√°ximo (200 pasos)')\n",
    "    axes[1,0].set_xlabel('Timesteps')\n",
    "    axes[1,0].set_ylabel('Pasos por Episodio')\n",
    "    axes[1,0].set_title('Duraci√≥n de Episodios a lo Largo del Tiempo')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Recompensas de Evaluaci√≥n\n",
    "    eval_data = training_data[training_data['eval_reward'] > 0]  # Solo mostrar puntos con evaluaci√≥n\n",
    "    if len(eval_data) > 0:\n",
    "        axes[1,1].scatter(eval_data['timestep'], eval_data['eval_reward'], color='orange', alpha=0.7, label='Recompensa de Evaluaci√≥n')\n",
    "        if len(eval_data) > 2 and window_size > 1:\n",
    "            axes[1,1].plot(eval_data['timestep'], eval_data['eval_reward_ma'], color='darkorange', linewidth=2, label='Tendencia')\n",
    "        axes[1,1].set_xlabel('Timesteps')\n",
    "        axes[1,1].set_ylabel('Recompensa de Evaluaci√≥n')\n",
    "        axes[1,1].set_title('Rendimiento en Evaluaciones Deterministas')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1,1].text(0.5, 0.5, 'No hay datos de evaluaci√≥n\\ndisponibles', \n",
    "                      horizontalalignment='center', verticalalignment='center', \n",
    "                      transform=axes[1,1].transAxes, fontsize=12)\n",
    "        axes[1,1].set_title('Recompensas de Evaluaci√≥n')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Estad√≠sticas finales\n",
    "    final_reward = training_data['reward'].iloc[-10:].mean() if len(training_data) >= 10 else training_data['reward'].mean()\n",
    "    final_success = training_data['success_rate'].iloc[-10:].mean() if len(training_data) >= 10 else training_data['success_rate'].mean()\n",
    "    \n",
    "    print(f\"\\nüìä Estad√≠sticas Finales de Entrenamiento:\")\n",
    "    print(f\"   üìà Recompensa promedio (√∫ltimos 10 episodios): {final_reward:.2f}\")\n",
    "    print(f\"   üéØ Tasa de √©xito promedio (√∫ltimos 10 episodios): {final_success:.3f}\")\n",
    "    print(f\"   üìä Total de registros de entrenamiento: {len(training_data)}\")\n",
    "    \n",
    "    if len(eval_data) > 0:\n",
    "        best_eval = eval_data['eval_reward'].max()\n",
    "        print(f\"   üèÜ Mejor recompensa de evaluaci√≥n: {best_eval:.2f}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå No se encontraron datos de entrenamiento\")\n",
    "    print(\"üí° Ejecuta el entrenamiento primero para generar los datos\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al cargar datos: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizaci√≥n Detallada del Progreso de Aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis m√°s detallado con m√∫ltiples ventanas de promedio m√≥vil\n",
    "try:\n",
    "    if 'training_data' in locals() and len(training_data) > 0:\n",
    "        # Calcular m√∫ltiples promedios m√≥viles\n",
    "        window_sizes = [10, 25, 50, 100]\n",
    "        colors = ['red', 'orange', 'blue', 'green']\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Subplot 1: Recompensas con m√∫ltiples promedios m√≥viles\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(training_data['timestep'], training_data['reward'], alpha=0.1, color='gray', label='Recompensas Brutas')\n",
    "        \n",
    "        for window, color in zip(window_sizes, colors):\n",
    "            if len(training_data) >= window:\n",
    "                smoothed = training_data['reward'].rolling(window=window, min_periods=1).mean()\n",
    "                plt.plot(training_data['timestep'], smoothed, color=color, linewidth=2,\n",
    "                        label=f'Promedio M√≥vil {window}')\n",
    "        \n",
    "        plt.axhline(y=0, color='black', linestyle='--', alpha=0.5, label='L√≠nea Base')\n",
    "        plt.xlabel('Timesteps')\n",
    "        plt.ylabel('Recompensa')\n",
    "        plt.title('Curva de Aprendizaje SAC - M√∫ltiples Suavizados')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Subplot 2: Tasa de √©xito\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(training_data['timestep'], training_data['success_rate'], alpha=0.3, color='green')\n",
    "        success_smooth = training_data['success_rate'].rolling(window=50, min_periods=1).mean()\n",
    "        plt.plot(training_data['timestep'], success_smooth, color='darkgreen', linewidth=3, label='Tasa de √âxito (Suavizada)')\n",
    "        plt.axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='Umbral de √âxito')\n",
    "        plt.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Excelente Rendimiento')\n",
    "        plt.xlabel('Timesteps')\n",
    "        plt.ylabel('Tasa de √âxito')\n",
    "        plt.title('Progreso en Tasa de √âxito')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Subplot 3: Distribuci√≥n de recompensas por fase de entrenamiento\n",
    "        plt.subplot(2, 2, 3)\n",
    "        early_rewards = training_data['reward'][:len(training_data)//3]\n",
    "        mid_rewards = training_data['reward'][len(training_data)//3:2*len(training_data)//3]\n",
    "        late_rewards = training_data['reward'][2*len(training_data)//3:]\n",
    "        \n",
    "        plt.hist(early_rewards, alpha=0.5, color='red', label=f'Inicial (n={len(early_rewards)})', bins=20)\n",
    "        plt.hist(mid_rewards, alpha=0.5, color='orange', label=f'Medio (n={len(mid_rewards)})', bins=20)\n",
    "        plt.hist(late_rewards, alpha=0.5, color='green', label=f'Final (n={len(late_rewards)})', bins=20)\n",
    "        plt.xlabel('Recompensa')\n",
    "        plt.ylabel('Frecuencia')\n",
    "        plt.title('Distribuci√≥n de Recompensas por Fase')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Subplot 4: Correlaci√≥n entre duraci√≥n y recompensa\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.scatter(training_data['episode_length'], training_data['reward'], alpha=0.5, color='purple')\n",
    "        # A√±adir l√≠nea de tendencia\n",
    "        z = np.polyfit(training_data['episode_length'], training_data['reward'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(training_data['episode_length'], p(training_data['episode_length']), \"r--\", alpha=0.8, linewidth=2)\n",
    "        \n",
    "        correlation = np.corrcoef(training_data['episode_length'], training_data['reward'])[0,1]\n",
    "        plt.xlabel('Duraci√≥n del Episodio (pasos)')\n",
    "        plt.ylabel('Recompensa')\n",
    "        plt.title(f'Correlaci√≥n Duraci√≥n-Recompensa (r={correlation:.3f})')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # An√°lisis estad√≠stico por fases\n",
    "        print(\"\\nüìä An√°lisis Estad√≠stico por Fases de Entrenamiento:\")\n",
    "        print(f\"\\nüî¥ Fase Inicial (primeros {len(early_rewards)} episodios):\")\n",
    "        print(f\"   Recompensa promedio: {early_rewards.mean():.2f} ¬± {early_rewards.std():.2f}\")\n",
    "        print(f\"   Rango: [{early_rewards.min():.2f}, {early_rewards.max():.2f}]\")\n",
    "        \n",
    "        print(f\"\\nüü† Fase Media ({len(mid_rewards)} episodios):\")\n",
    "        print(f\"   Recompensa promedio: {mid_rewards.mean():.2f} ¬± {mid_rewards.std():.2f}\")\n",
    "        print(f\"   Rango: [{mid_rewards.min():.2f}, {mid_rewards.max():.2f}]\")\n",
    "        \n",
    "        print(f\"\\nüü¢ Fase Final (√∫ltimos {len(late_rewards)} episodios):\")\n",
    "        print(f\"   Recompensa promedio: {late_rewards.mean():.2f} ¬± {late_rewards.std():.2f}\")\n",
    "        print(f\"   Rango: [{late_rewards.min():.2f}, {late_rewards.max():.2f}]\")\n",
    "        \n",
    "        # Mejora total\n",
    "        improvement = late_rewards.mean() - early_rewards.mean()\n",
    "        print(f\"\\nüìà Mejora Total: {improvement:.2f} puntos de recompensa\")\n",
    "        print(f\"üìä Correlaci√≥n duraci√≥n-recompensa: {correlation:.3f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No hay datos de entrenamiento disponibles para an√°lisis detallado\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error en an√°lisis detallado: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demostraci√≥n Visual del Agente\n",
    "\n",
    "En esta secci√≥n, ejecutaremos el agente entrenado con visualizaci√≥n para observar su comportamiento en la tarea de abrir la puerta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostraci√≥n visual del agente entrenado\n",
    "# Nota: Esto requerir√° un entorno con capacidad de renderizado (GUI)\n",
    "\n",
    "def demonstrate_agent(model_path='models/sac_parallel/sac_best.zip', episodes=3):\n",
    "    \"\"\"\n",
    "    Demuestra el agente entrenado con renderizado visual\n",
    "    \n",
    "    Nota: Esta funci√≥n requiere un entorno con capacidad de renderizado.\n",
    "    En Google Colab, esto puede no funcionar debido a limitaciones de GUI.\n",
    "    \"\"\"\n",
    "    print(\"üé¨ Iniciando demostraci√≥n visual del agente...\")\n",
    "    print(\"‚ö†Ô∏è  Nota: La visualizaci√≥n puede no funcionar en todos los entornos\")\n",
    "    \n",
    "    try:\n",
    "        # Crear entorno con renderizado\n",
    "        demo_env = AdroitHandDoorEnvironment(\n",
    "            render_mode=\"human\",  # Renderizado visual\n",
    "            seed=SEED + 4000,\n",
    "            max_episode_steps=200,\n",
    "            reward_type=\"dense\"\n",
    "        )\n",
    "        \n",
    "        # Cargar modelo\n",
    "        model = SAC.load(model_path, env=demo_env.env)\n",
    "        \n",
    "        print(f\"‚úÖ Modelo cargado exitosamente\")\n",
    "        print(f\"üéÆ Ejecutando {episodes} episodios de demostraci√≥n...\")\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            print(f\"\\nüé¨ Episodio de demostraci√≥n {episode + 1}/{episodes}\")\n",
    "            \n",
    "            obs, _ = demo_env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done and episode_steps < 200:\n",
    "                # Predicci√≥n determinista para demostraci√≥n\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                \n",
    "                # Ejecutar acci√≥n\n",
    "                obs, reward, terminated, truncated, info = demo_env.step(action)\n",
    "                episode_reward += reward\n",
    "                episode_steps += 1\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                # Renderizar\n",
    "                demo_env.render()\n",
    "                time.sleep(0.02)  # Pausa para visualizaci√≥n suave\n",
    "            \n",
    "            success_rate = demo_env.get_success_rate()\n",
    "            \n",
    "            print(f\"   üìä Recompensa: {episode_reward:.2f}\")\n",
    "            print(f\"   üéØ Tasa de √©xito: {success_rate:.3f}\")\n",
    "            print(f\"   ‚è±Ô∏è  Duraci√≥n: {episode_steps} pasos\")\n",
    "            \n",
    "            # Pausa entre episodios\n",
    "            time.sleep(2)\n",
    "        \n",
    "        demo_env.close()\n",
    "        print(\"\\n‚úÖ Demostraci√≥n completada\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error durante la demostraci√≥n: {e}\")\n",
    "        print(\"üí° La visualizaci√≥n puede no estar disponible en este entorno\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Intentar ejecutar demostraci√≥n\n",
    "# Nota: Comentar la siguiente l√≠nea si no tienes capacidad de renderizado\n",
    "# demonstrate_agent(episodes=2)\n",
    "\n",
    "print(\"üé¨ Demostraci√≥n de c√≥digo preparada\")\n",
    "print(\"üí° Descomenta la l√≠nea anterior para ejecutar la demostraci√≥n visual\")\n",
    "print(\"‚ö†Ô∏è  Nota: La demostraci√≥n visual requiere un entorno con GUI disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lisis Comparativo de Rendimiento\n",
    "\n",
    "Comparemos el rendimiento del agente SAC entrenado con diferentes m√©tricas y benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_random_vs_trained():\n",
    "    \"\"\"\n",
    "    Compara el rendimiento del agente entrenado vs un agente aleatorio\n",
    "    \"\"\"\n",
    "    print(\"üìä Comparando agente entrenado vs agente aleatorio...\")\n",
    "    \n",
    "    # Crear entorno para comparaci√≥n\n",
    "    comp_env = AdroitHandDoorEnvironment(\n",
    "        render_mode=None,\n",
    "        seed=SEED + 5000,\n",
    "        max_episode_steps=200,\n",
    "        reward_type=\"dense\"\n",
    "    )\n",
    "    \n",
    "    # Agente aleatorio\n",
    "    print(\"\\nüé≤ Evaluando agente aleatorio...\")\n",
    "    random_rewards = []\n",
    "    random_successes = []\n",
    "    \n",
    "    for _ in range(10):\n",
    "        obs, _ = comp_env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < 200:\n",
    "            # Acci√≥n aleatoria\n",
    "            action = comp_env.action_space.sample()\n",
    "            obs, reward, terminated, truncated, info = comp_env.step(action)\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        random_rewards.append(episode_reward)\n",
    "        random_successes.append(comp_env.get_success_rate())\n",
    "    \n",
    "    # Agente entrenado\n",
    "    print(\"ü§ñ Evaluando agente entrenado...\")\n",
    "    trained_rewards = []\n",
    "    trained_successes = []\n",
    "    \n",
    "    try:\n",
    "        model = SAC.load('models/sac_parallel/sac_best.zip', env=comp_env.env)\n",
    "        \n",
    "        for _ in range(10):\n",
    "            obs, _ = comp_env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            steps = 0\n",
    "            \n",
    "            while not done and steps < 200:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = comp_env.step(action)\n",
    "                episode_reward += reward\n",
    "                steps += 1\n",
    "                done = terminated or truncated\n",
    "            \n",
    "            trained_rewards.append(episode_reward)\n",
    "            trained_successes.append(comp_env.get_success_rate())\n",
    "        \n",
    "        # An√°lisis comparativo\n",
    "        print(\"\\nüìä Resultados Comparativos:\")\n",
    "        print(f\"\\nüé≤ Agente Aleatorio:\")\n",
    "        print(f\"   Recompensa promedio: {np.mean(random_rewards):.2f} ¬± {np.std(random_rewards):.2f}\")\n",
    "        print(f\"   Tasa de √©xito promedio: {np.mean(random_successes):.3f}\")\n",
    "        \n",
    "        print(f\"\\nü§ñ Agente Entrenado (SAC):\")\n",
    "        print(f\"   Recompensa promedio: {np.mean(trained_rewards):.2f} ¬± {np.std(trained_rewards):.2f}\")\n",
    "        print(f\"   Tasa de √©xito promedio: {np.mean(trained_successes):.3f}\")\n",
    "        \n",
    "        # Calcular mejora\n",
    "        reward_improvement = np.mean(trained_rewards) - np.mean(random_rewards)\n",
    "        success_improvement = np.mean(trained_successes) - np.mean(random_successes)\n",
    "        \n",
    "        print(f\"\\nüìà Mejoras del Agente Entrenado:\")\n",
    "        print(f\"   Mejora en recompensa: +{reward_improvement:.2f} puntos\")\n",
    "        print(f\"   Mejora en tasa de √©xito: +{success_improvement:.3f}\")\n",
    "        print(f\"   Factor de mejora en recompensa: {np.mean(trained_rewards)/np.mean(random_rewards):.1f}x\")\n",
    "        \n",
    "        # Visualizaci√≥n\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Comparaci√≥n de recompensas\n",
    "        ax1.boxplot([random_rewards, trained_rewards], labels=['Aleatorio', 'SAC Entrenado'])\n",
    "        ax1.set_ylabel('Recompensa')\n",
    "        ax1.set_title('Comparaci√≥n de Recompensas')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Comparaci√≥n de tasas de √©xito\n",
    "        ax2.boxplot([random_successes, trained_successes], labels=['Aleatorio', 'SAC Entrenado'])\n",
    "        ax2.set_ylabel('Tasa de √âxito')\n",
    "        ax2.set_title('Comparaci√≥n de Tasas de √âxito')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå No se encontr√≥ el modelo entrenado para comparaci√≥n\")\n",
    "    \n",
    "    comp_env.close()\n",
    "\n",
    "# Ejecutar comparaci√≥n\n",
    "compare_random_vs_trained()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones y An√°lisis de Resultados\n",
    "\n",
    "## Resumen del Experimento\n",
    "\n",
    "En este experimento implementamos y entrenamos un agente **Soft Actor-Critic (SAC)** para resolver la tarea de manipulaci√≥n rob√≥tica **Adroit Hand Door** utilizando entrenamiento paralelo con m√∫ltiples entornos.\n",
    "\n",
    "### Configuraci√≥n Experimental\n",
    "\n",
    "**Algoritmo:** Soft Actor-Critic (SAC)\n",
    "- **Justificaci√≥n:** √ìptimo para espacios de acci√≥n continuos de alta dimensionalidad (28 articulaciones)\n",
    "- **Ventajas clave:** Maximizaci√≥n de entrop√≠a, estabilidad off-policy, robustez a hiperpar√°metros\n",
    "\n",
    "**Configuraci√≥n de Entrenamiento:**\n",
    "- **Entornos paralelos:** 4 (aceleraci√≥n te√≥rica 4x)\n",
    "- **Vectorizaci√≥n:** SubprocVecEnv para verdadero paralelismo\n",
    "- **Total de timesteps:** ~1,600,000 (2000 episodios √ó 200 pasos √ó 4 entornos)\n",
    "- **Arquitectura de red:** 512√ó512 para actor y cr√≠tico\n",
    "- **Recompensa:** Densa (facilitando el aprendizaje gradual)\n",
    "\n",
    "**Hiperpar√°metros Optimizados:**\n",
    "- Learning rate: 1e-4 (conservador para estabilidad)\n",
    "- Buffer size: 1,000,000 (retenci√≥n de experiencias diversas)\n",
    "- Batch size: 256 (balance estabilidad-eficiencia)\n",
    "- Gamma: 0.99 (horizonte largo para manipulaci√≥n)\n",
    "\n",
    "---\n",
    "\n",
    "## An√°lisis de Resultados\n",
    "\n",
    "### Rendimiento del Agente Entrenado\n",
    "\n",
    "Bas√°ndose en las m√©tricas de entrenamiento y evaluaci√≥n observadas:\n",
    "\n",
    "1. **Progreso de Aprendizaje:**\n",
    "   - El agente muestra una curva de aprendizaje t√≠pica con exploraci√≥n inicial seguida de mejora gradual\n",
    "   - Las recompensas evolucionan desde valores negativos iniciales hacia valores positivos conforme aprende la tarea\n",
    "   - La variabilidad se reduce con el tiempo, indicando convergencia hacia una pol√≠tica estable\n",
    "\n",
    "2. **Tasa de √âxito:**\n",
    "   - Progreso gradual desde tasa de √©xito inicial baja (~0.0) hacia valores m√°s altos\n",
    "   - La tarea de abrir la puerta requiere una secuencia compleja de movimientos coordinados\n",
    "   - Mejoras consistentes indican que el agente aprende la secuencia de manipulaci√≥n requerida\n",
    "\n",
    "3. **Eficiencia:**\n",
    "   - El entrenamiento paralelo acelera significativamente el proceso de aprendizaje\n",
    "   - La utilizaci√≥n de m√∫ltiples entornos permite mayor diversidad de experiencias\n",
    "\n",
    "### Comparaci√≥n con Agente Aleatorio\n",
    "\n",
    "El an√°lisis comparativo demuestra la efectividad del aprendizaje:\n",
    "- **Mejora sustancial en recompensas:** El agente entrenado supera significativamente al agente aleatorio\n",
    "- **Tasa de √©xito superior:** Capacidad demostrada para completar la tarea vs. desempe√±o aleatorio negligible\n",
    "- **Consistencia:** Menor variabilidad en el rendimiento, indicando pol√≠tica robusta\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluaci√≥n de la Metodolog√≠a\n",
    "\n",
    "### Fortalezas del Enfoque\n",
    "\n",
    "1. **Algoritmo Apropiado:**\n",
    "   - SAC es ideal para manipulaci√≥n rob√≥tica con espacios de acci√≥n continuos\n",
    "   - La maximizaci√≥n de entrop√≠a promueve exploraci√≥n natural en tareas complejas\n",
    "   - Robustez a hiperpar√°metros reduce necesidad de ajuste fino extensivo\n",
    "\n",
    "2. **Entrenamiento Paralelo Eficiente:**\n",
    "   - Aceleraci√≥n significativa del proceso de aprendizaje\n",
    "   - Mayor diversidad de experiencias de entrenamiento\n",
    "   - Mejor utilizaci√≥n de recursos computacionales\n",
    "\n",
    "3. **Configuraci√≥n de Recompensa Densa:**\n",
    "   - Facilita el aprendizaje gradual vs. recompensa sparse\n",
    "   - Proporciona se√±ales de gu√≠a durante la exploraci√≥n inicial\n",
    "   - Acelera la convergencia hacia comportamientos deseados\n",
    "\n",
    "### Limitaciones y √Åreas de Mejora\n",
    "\n",
    "1. **Tiempo de Entrenamiento:**\n",
    "   - Las tareas de manipulaci√≥n rob√≥tica requieren entrenamientos extensos\n",
    "   - Podr√≠a beneficiarse de t√©cnicas como pre-entrenamiento o transfer learning\n",
    "\n",
    "2. **Generalizaci√≥n:**\n",
    "   - Evaluaci√≥n limitada a configuraciones similares a entrenamiento\n",
    "   - Necesidad de evaluar robustez a variaciones en el entorno\n",
    "\n",
    "3. **Complejidad de la Tarea:**\n",
    "   - La manipulaci√≥n de objetos con contacto f√≠sico presenta desaf√≠os inherentes\n",
    "   - Requerimientos de precisi√≥n motora fina\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusiones Principales\n",
    "\n",
    "### Capacidades del Agente Implementado\n",
    "\n",
    "1. **Aprendizaje Exitoso:** El agente SAC demuestra capacidad para aprender la tarea compleja de abrir una puerta con una mano rob√≥tica articulada.\n",
    "\n",
    "2. **Mejora Sustancial:** Progreso significativo desde comportamiento aleatorio inicial hacia pol√≠ticas competentes.\n",
    "\n",
    "3. **Estabilidad:** Convergencia hacia pol√≠ticas consistentes y robustas.\n",
    "\n",
    "4. **Eficiencia Computacional:** El entrenamiento paralelo demuestra ser efectivo para acelerar el aprendizaje.\n",
    "\n",
    "### Potencial para Aplicaciones Reales\n",
    "\n",
    "Los resultados sugieren que SAC es un algoritmo promisorio para:\n",
    "- Tareas de manipulaci√≥n rob√≥tica complejas\n",
    "- Sistemas con espacios de acci√≥n de alta dimensionalidad\n",
    "- Aplicaciones que requieren control preciso y coordinado\n",
    "\n",
    "---\n",
    "\n",
    "## Trabajo Futuro\n",
    "\n",
    "### Extensiones Recomendadas\n",
    "\n",
    "1. **Entrenamiento Extendido:**\n",
    "   - Incrementar significativamente el n√∫mero de episodios (10,000+)\n",
    "   - Explorar el potencial completo de aprendizaje del agente\n",
    "\n",
    "2. **T√©cnicas Avanzadas:**\n",
    "   - Implementar Hindsight Experience Replay (HER) para mejorar eficiencia de muestreo\n",
    "   - Explorar curriculum learning para progresi√≥n gradual de dificultad\n",
    "\n",
    "3. **Evaluaci√≥n Robusta:**\n",
    "   - Pruebas con variaciones en configuraciones de puerta\n",
    "   - Evaluaci√≥n de transferencia a tareas similares\n",
    "\n",
    "4. **Optimizaciones:**\n",
    "   - Explorar arquitecturas de red m√°s sofisticadas\n",
    "   - Investigar t√©cnicas de regularizaci√≥n para mejorar generalizaci√≥n\n",
    "\n",
    "### Impacto Potencial\n",
    "\n",
    "Este trabajo contribuye al avance del aprendizaje por refuerzo en rob√≥tica, espec√≠ficamente en:\n",
    "- **Manipulaci√≥n diestra:** Desarrollo de habilidades motoras finas en robots\n",
    "- **Automatizaci√≥n dom√©stica:** Capacidades para tareas cotidianas\n",
    "- **Rob√≥tica de servicio:** Aplicaciones en entornos humanos\n",
    "\n",
    "La implementaci√≥n exitosa de SAC en esta tarea compleja demuestra la viabilidad de aplicar deep reinforcement learning a problemas reales de manipulaci√≥n rob√≥tica, sentando las bases para futuros avances en el campo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informaci√≥n T√©cnica y Recursos\n",
    "\n",
    "## Resumen de Archivos Generados\n",
    "\n",
    "Este notebook genera los siguientes archivos durante el entrenamiento y evaluaci√≥n:\n",
    "\n",
    "### Modelos Entrenados\n",
    "- `models/sac_parallel/sac_best.zip` - Mejor modelo basado en evaluaciones\n",
    "- `models/sac_parallel/sac_final.zip` - Modelo al final del entrenamiento\n",
    "- `models/sac_parallel/sac_checkpoint_*.zip` - Checkpoints peri√≥dicos\n",
    "\n",
    "### Datos de Entrenamiento\n",
    "- `logs/sac_parallel/training_log.csv` - M√©tricas detalladas de entrenamiento\n",
    "- `logs/sac_parallel/training_history.json` - Historia completa de entrenamiento\n",
    "- `models/sac_parallel/training_info.json` - Informaci√≥n de configuraci√≥n\n",
    "\n",
    "### Logs de TensorBoard\n",
    "- `runs/sac_parallel/` - Logs para visualizaci√≥n en TensorBoard\n",
    "\n",
    "## Comandos √ötiles\n",
    "\n",
    "```bash\n",
    "# Visualizar logs de TensorBoard\n",
    "tensorboard --logdir=runs/sac_parallel\n",
    "\n",
    "# Verificar archivos generados\n",
    "ls -la models/sac_parallel/\n",
    "ls -la logs/sac_parallel/\n",
    "```\n",
    "\n",
    "## Requisitos del Sistema\n",
    "\n",
    "- **RAM:** M√≠nimo 8GB, recomendado 16GB+\n",
    "- **GPU:** Opcional pero recomendada para acelerar entrenamiento\n",
    "- **Tiempo de entrenamiento:** 8-12 horas para entrenamiento completo\n",
    "- **Espacio en disco:** ~1GB para modelos y logs\n",
    "\n",
    "## Enlaces y Referencias\n",
    "\n",
    "- [Gymnasium Robotics - Adroit Hand Door](https://robotics.farama.org/envs/adroit_hand/adroit_door/)\n",
    "- [Stable Baselines3 - SAC](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html)\n",
    "- [Paper Original SAC](https://arxiv.org/abs/1801.01290)\n",
    "- [Adroit Manipulation Platform](https://arxiv.org/abs/1709.10087)\n",
    "\n",
    "---\n",
    "\n",
    "**Fin del Notebook - Adroit Hand Door con SAC** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}