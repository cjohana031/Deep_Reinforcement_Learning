{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shadow Dexterous Hand Reach - SAC Implementation\n",
    "\n",
    "**Fecha:** Septiembre 26, 2025\n",
    "\n",
    "## 1 Introducci√≥n\n",
    "\n",
    "Este notebook implementa un Soft Actor-Critic (SAC) para resolver la tarea Shadow Dexterous Hand Reach del conjunto gymnasium-robotics utilizando el entorno `HandReachDense-v2`. El objetivo es entrenar un agente que aprenda a controlar una mano rob√≥tica de Shadow con 24 articulaciones para alcanzar posiciones objetivo con las 5 puntas de los dedos.\n",
    "\n",
    "## 2 El Problema: Shadow Dexterous Hand Reach\n",
    "\n",
    "Shadow Dexterous Hand Reach es una tarea de manipulaci√≥n rob√≥tica compleja donde el agente debe:\n",
    "\n",
    "‚Ä¢ **Controlar una mano rob√≥tica Shadow** con 24 grados de libertad distribuidos en 5 dedos\n",
    "‚Ä¢ **Alcanzar posiciones objetivo** con todas las puntas de los dedos simult√°neamente\n",
    "‚Ä¢ **Manejar coordinaci√≥n compleja** entre m√∫ltiples articulaciones para lograr precisi√≥n\n",
    "‚Ä¢ **Optimizar trayectorias** en un espacio de alta dimensionalidad\n",
    "‚Ä¢ **Adaptarse a objetivos randomizados** en cada episodio\n",
    "\n",
    "### 2.1 Especificaciones T√©cnicas\n",
    "\n",
    "#### 2.1.1 Espacio de Acciones\n",
    "‚Ä¢ **Tipo:** Continuo Box(-1.0, 1.0, (20,), float32)\n",
    "‚Ä¢ **Dimensiones:** 20 acciones correspondientes a √°ngulos de articulaciones\n",
    "  - Acciones 0-3: Articulaciones del pulgar\n",
    "  - Acciones 4-7: Articulaciones del dedo √≠ndice\n",
    "  - Acciones 8-11: Articulaciones del dedo medio\n",
    "  - Acciones 12-15: Articulaciones del dedo anular\n",
    "  - Acciones 16-19: Articulaciones del dedo me√±ique\n",
    "\n",
    "#### 2.1.2 Espacio de Observaciones\n",
    "‚Ä¢ **Tipo:** Dict con m√∫ltiples componentes\n",
    "‚Ä¢ **Dimensiones totales:** 93 observaciones que incluyen:\n",
    "  - **observation (63):** Estado de articulaciones y cinem√°tica de la mano\n",
    "  - **achieved_goal (15):** Posiciones actuales de las 5 puntas de dedos (x,y,z cada una)\n",
    "  - **desired_goal (15):** Posiciones objetivo de las 5 puntas de dedos (x,y,z cada una)\n",
    "\n",
    "#### 2.1.3 Sistema de Recompensas (Versi√≥n Densa)\n",
    "La recompensa densa se basa en la distancia L2 negativa entre posiciones alcanzadas y objetivos:\n",
    "\n",
    "1. **Distance reward:** -||achieved_goal - desired_goal||‚ÇÇ\n",
    "2. **Success bonus:** Recompensa adicional cuando todas las puntas est√°n cerca del objetivo\n",
    "3. **Shaped reward:** Incentivos graduales para aproximarse a los objetivos\n",
    "\n",
    "#### 2.1.4 Estados Finales\n",
    "‚Ä¢ **Truncamiento:** Despu√©s de 50 pasos (configurable con max_episode_steps)\n",
    "‚Ä¢ **√âxito:** Cuando todas las puntas de dedos est√°n dentro de un umbral de distancia del objetivo\n",
    "‚Ä¢ **Episodios cortos:** Optimizados para aprendizaje eficiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias requeridas\n",
    "!pip install gymnasium-robotics stable-baselines3[extra] tensorboard opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "import os\n",
    "\n",
    "# Stable Baselines3 imports\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Register gymnasium robotics environments\n",
    "gym.register_envs(gymnasium_robotics)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(\"‚úÖ Todas las dependencias importadas correctamente\")\n",
    "print(f\"üå± Semilla aleatoria establecida: {SEED}\")\n",
    "print(f\"üñ•Ô∏è Dispositivo PyTorch: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 ¬øPor qu√© SAC?\n",
    "\n",
    "Se escogi√≥ Soft Actor-Critic (SAC) como m√©todo principal para este proyecto debido a sus ventajas espec√≠ficas para tareas de manipulaci√≥n rob√≥tica con espacios de acci√≥n continuos y alta dimensionalidad.\n",
    "\n",
    "### 3.1 Ventajas de SAC para Shadow Dexterous Hand Reach\n",
    "\n",
    "‚Ä¢ **Espacios de Acci√≥n Continuos:** Las 20 articulaciones de la mano requieren control continuo preciso. SAC est√° espec√≠ficamente dise√±ado para manejar espacios de acci√≥n continuos de alta dimensionalidad.\n",
    "\n",
    "‚Ä¢ **Maximizaci√≥n de Entrop√≠a:** SAC maximiza tanto la recompensa como la entrop√≠a de la pol√≠tica, promoviendo exploraci√≥n natural. Esto es crucial para tareas de manipulaci√≥n donde hay m√∫ltiples formas de alcanzar el objetivo.\n",
    "\n",
    "‚Ä¢ **Estabilidad en Off-Policy Learning:** SAC es un algoritmo off-policy que puede reutilizar experiencias pasadas eficientemente, crucial para entornos donde la interacci√≥n es computacionalmente costosa.\n",
    "\n",
    "‚Ä¢ **Robustez a Hiperpar√°metros:** SAC es conocido por ser robusto a la selecci√≥n de hiperpar√°metros, reduciendo la necesidad de ajuste fino extensivo.\n",
    "\n",
    "‚Ä¢ **Paralelizaci√≥n Eficiente:** SAC se beneficia significativamente del entrenamiento paralelo con m√∫ltiples entornos, acelerando el aprendizaje.\n",
    "\n",
    "‚Ä¢ **Manejo de Recompensas Densas:** La estructura de recompensa densa de Shadow Hand Reach se alinea bien con el aprendizaje continuo que SAC proporciona.\n",
    "\n",
    "### 3.2 Comparaci√≥n con Otros Algoritmos\n",
    "\n",
    "| Algoritmo | Tipo | Ventajas para esta tarea | Desventajas |\n",
    "|-----------|------|-------------------------|-------------|\n",
    "| **SAC** | Off-policy, Actor-Critic | ‚úÖ Acciones continuas, exploraci√≥n autom√°tica, estable | Computacionalmente intensivo |\n",
    "| **PPO** | On-policy, Actor-Critic | ‚úÖ Estable, f√°cil de implementar | ‚ùå Menos eficiente en muestreo, exploraci√≥n limitada |\n",
    "| **TD3** | Off-policy, Actor-Critic | ‚úÖ Acciones continuas, eficiente | ‚ùå Requiere m√°s ajuste de hiperpar√°metros |\n",
    "| **DDPG** | Off-policy, Actor-Critic | ‚úÖ Acciones continuas | ‚ùå Menos estable, sensible a hiperpar√°metros |\n",
    "| **A3C** | On-policy, Actor-Critic | ‚úÖ Paralelizaci√≥n natural | ‚ùå On-policy, exploraci√≥n sub√≥ptima para manipulaci√≥n |\n",
    "\n",
    "### 3.3 Arquitectura del Sistema\n",
    "\n",
    "#### 3.3.1 Componentes Principales\n",
    "\n",
    "1. **ShadowHandReachEnvironment:** Wrapper del entorno gymnasium-robotics\n",
    "2. **SB3SACAgent:** Agente SAC basado en Stable Baselines3  \n",
    "3. **TrainingCallback:** Callback personalizado para evaluaci√≥n y guardado\n",
    "4. **Logger:** Sistema de logging y monitoreo de m√©tricas\n",
    "5. **Entrenamiento Paralelo:** Uso de m√∫ltiples entornos para acelerar el aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShadowHandReachEnvironment:\n",
    "    \"\"\"Wrapper para el entorno Shadow Hand Reach con funcionalidades adicionales\"\"\"\n",
    "    \n",
    "    def __init__(self, render_mode=None, seed=42, max_episode_steps=50, reward_type=\"dense\"):\n",
    "        \"\"\"\n",
    "        Inicializa el entorno Shadow Hand Reach\n",
    "        \n",
    "        Args:\n",
    "            render_mode: Modo de renderizado ('human', 'rgb_array', None)\n",
    "            seed: Semilla para reproducibilidad\n",
    "            max_episode_steps: M√°ximo n√∫mero de pasos por episodio\n",
    "            reward_type: Tipo de recompensa ('dense' o 'sparse')\n",
    "        \"\"\"\n",
    "        self.render_mode = render_mode\n",
    "        self.seed = seed\n",
    "        self.max_episode_steps = max_episode_steps\n",
    "        self.reward_type = reward_type\n",
    "        \n",
    "        # Crear el entorno base\n",
    "        env_id = f\"HandReachDense-v2\"\n",
    "        self.env = gym.make(\n",
    "            env_id,\n",
    "            max_episode_steps=max_episode_steps,\n",
    "            render_mode=render_mode\n",
    "        )\n",
    "        \n",
    "        # Configurar semilla\n",
    "        if seed is not None:\n",
    "            self.env.reset(seed=seed)\n",
    "        \n",
    "        # Informaci√≥n del entorno\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.action_space = self.env.action_space\n",
    "        \n",
    "        # M√©tricas de seguimiento\n",
    "        self.episode_steps = 0\n",
    "        self.episode_reward = 0\n",
    "        self.success_threshold = 0.05  # Umbral de distancia para considerar √©xito\n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reinicia el entorno\"\"\"\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "        self.episode_steps = 0\n",
    "        self.episode_reward = 0\n",
    "        return obs, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Ejecuta una acci√≥n en el entorno\"\"\"\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.episode_steps += 1\n",
    "        self.episode_reward += reward\n",
    "        \n",
    "        # Agregar informaci√≥n adicional\n",
    "        info['episode_steps'] = self.episode_steps\n",
    "        info['episode_reward'] = self.episode_reward\n",
    "        info['success'] = self.get_success_rate(obs)\n",
    "        info['distance'] = self.get_goal_distance(obs)\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "    def get_success_rate(self, obs=None):\n",
    "        \"\"\"Calcula la tasa de √©xito basada en la distancia al objetivo\"\"\"\n",
    "        try:\n",
    "            if obs is None:\n",
    "                obs, _ = self.env.reset()\n",
    "            \n",
    "            # Extraer achieved_goal y desired_goal\n",
    "            achieved_goal = obs['achieved_goal']\n",
    "            desired_goal = obs['desired_goal']\n",
    "            \n",
    "            # Calcular distancia L2\n",
    "            distance = np.linalg.norm(achieved_goal - desired_goal)\n",
    "            \n",
    "            # √âxito si la distancia est√° por debajo del umbral\n",
    "            success = 1.0 if distance < self.success_threshold else 0.0\n",
    "            return success\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def get_goal_distance(self, obs=None):\n",
    "        \"\"\"Calcula la distancia actual al objetivo\"\"\"\n",
    "        try:\n",
    "            if obs is None:\n",
    "                obs, _ = self.env.reset()\n",
    "            \n",
    "            achieved_goal = obs['achieved_goal']\n",
    "            desired_goal = obs['desired_goal']\n",
    "            \n",
    "            return np.linalg.norm(achieved_goal - desired_goal)\n",
    "        except:\n",
    "            return float('inf')\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Renderiza el entorno\"\"\"\n",
    "        return self.env.render()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Cierra el entorno\"\"\"\n",
    "        self.env.close()\n",
    "    \n",
    "    def get_state_dim(self):\n",
    "        \"\"\"Retorna la dimensi√≥n del espacio de estados\"\"\"\n",
    "        # Para entornos dict, sumar todas las dimensiones\n",
    "        total_dim = 0\n",
    "        for key, space in self.observation_space.spaces.items():\n",
    "            total_dim += space.shape[0]\n",
    "        return total_dim\n",
    "    \n",
    "    def get_action_dim(self):\n",
    "        \"\"\"Retorna la dimensi√≥n del espacio de acciones\"\"\"\n",
    "        return self.action_space.shape[0]\n",
    "    \n",
    "    def get_action_bounds(self):\n",
    "        \"\"\"Retorna los l√≠mites del espacio de acciones\"\"\"\n",
    "        return (self.action_space.low, self.action_space.high)\n",
    "\n",
    "\n",
    "class SuccessInfoWrapper(gym.Wrapper):\n",
    "    \"\"\"Wrapper para agregar informaci√≥n de √©xito compatible con SB3\"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.success_threshold = 0.05\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Calcular √©xito basado en distancia al objetivo\n",
    "        if isinstance(obs, dict) and 'achieved_goal' in obs and 'desired_goal' in obs:\n",
    "            distance = np.linalg.norm(obs['achieved_goal'] - obs['desired_goal'])\n",
    "            info['is_success'] = distance < self.success_threshold\n",
    "            info['distance'] = distance\n",
    "        else:\n",
    "            info['is_success'] = False\n",
    "            info['distance'] = float('inf')\n",
    "        \n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "# Crear una instancia de prueba para verificar las dimensiones\n",
    "test_env = ShadowHandReachEnvironment(render_mode=None, seed=SEED)\n",
    "print(f\"‚úÖ Entorno Shadow Hand Reach creado exitosamente\")\n",
    "print(f\"üìè Dimensiones del estado: {test_env.get_state_dim()}\")\n",
    "print(f\"üéÆ Dimensiones de la acci√≥n: {test_env.get_action_dim()}\")\n",
    "print(f\"üìä L√≠mites de acci√≥n: {test_env.get_action_bounds()[0][:5]}... a {test_env.get_action_bounds()[1][:5]}...\")\n",
    "print(f\"üîç Espacio de observaci√≥n: {test_env.observation_space}\")\n",
    "print(f\"üéØ Espacio de acci√≥n: {test_env.action_space}\")\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, el entorno Shadow Hand Reach tiene:\n",
    "- **93 dimensiones de estado total:** Incluyendo observaciones de articulaciones (63), objetivos alcanzados (15), y objetivos deseados (15)\n",
    "- **20 dimensiones de acci√≥n:** Correspondientes a las 20 articulaciones de la mano\n",
    "- **Espacio de acci√≥n continuo:** Valores entre -1 y 1 para cada articulaci√≥n\n",
    "\n",
    "Esta alta dimensionalidad y la naturaleza del control de objetivos m√∫ltiples hace que SAC sea una elecci√≥n ideal debido a su capacidad para manejar espacios de acci√≥n continuos complejos y su robustez en tareas de manipulaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "    \"\"\"Sistema de logging para m√©tricas de entrenamiento\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir='logs/sac_shadow_hand', tensorboard_dir='runs/sac_shadow_hand'):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.tensorboard_dir = Path(tensorboard_dir)\n",
    "        \n",
    "        # Crear directorios\n",
    "        self.log_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.tensorboard_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Historia de m√©tricas\n",
    "        self.history = {\n",
    "            'timesteps': [],\n",
    "            'episodes': [],\n",
    "            'rewards': [],\n",
    "            'episode_lengths': [],\n",
    "            'success_rates': [],\n",
    "            'distances': [],\n",
    "            'eval_rewards': [],\n",
    "            'eval_success_rates': [],\n",
    "            'timestamps': []\n",
    "        }\n",
    "        \n",
    "        # Inicializar CSV\n",
    "        self.csv_path = self.log_dir / 'training_log.csv'\n",
    "        self._init_csv()\n",
    "    \n",
    "    def _init_csv(self):\n",
    "        \"\"\"Inicializa el archivo CSV con headers\"\"\"\n",
    "        with open(self.csv_path, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\n",
    "                'timestep', 'episode', 'reward', 'episode_length',\n",
    "                'success_rate', 'distance', 'eval_reward', 'eval_success_rate', 'timestamp'\n",
    "            ])\n",
    "    \n",
    "    def log_episode(self, timestep, episode, reward, episode_length, \n",
    "                   success_rate=None, distance=None, eval_reward=None, eval_success_rate=None):\n",
    "        \"\"\"Registra m√©tricas de un episodio\"\"\"\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        # Agregar a historia\n",
    "        self.history['timesteps'].append(timestep)\n",
    "        self.history['episodes'].append(episode)\n",
    "        self.history['rewards'].append(reward)\n",
    "        self.history['episode_lengths'].append(episode_length)\n",
    "        self.history['success_rates'].append(success_rate or 0.0)\n",
    "        self.history['distances'].append(distance or 0.0)\n",
    "        self.history['eval_rewards'].append(eval_reward or 0.0)\n",
    "        self.history['eval_success_rates'].append(eval_success_rate or 0.0)\n",
    "        self.history['timestamps'].append(timestamp)\n",
    "        \n",
    "        # Escribir a CSV\n",
    "        with open(self.csv_path, 'a', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow([\n",
    "                timestep, episode, reward, episode_length,\n",
    "                success_rate or 0.0, distance or 0.0,\n",
    "                eval_reward or 0.0, eval_success_rate or 0.0, timestamp\n",
    "            ])\n",
    "    \n",
    "    def get_recent_stats(self, n=100):\n",
    "        \"\"\"Obtiene estad√≠sticas de los √∫ltimos n episodios\"\"\"\n",
    "        if len(self.history['rewards']) < n:\n",
    "            n = len(self.history['rewards'])\n",
    "        if n == 0:\n",
    "            return {}\n",
    "        \n",
    "        recent_rewards = self.history['rewards'][-n:]\n",
    "        recent_success = self.history['success_rates'][-n:]\n",
    "        recent_distances = self.history['distances'][-n:]\n",
    "        \n",
    "        return {\n",
    "            'mean_reward': np.mean(recent_rewards),\n",
    "            'std_reward': np.std(recent_rewards),\n",
    "            'mean_success': np.mean(recent_success),\n",
    "            'mean_distance': np.mean(recent_distances),\n",
    "            'episodes_logged': len(self.history['episodes'])\n",
    "        }\n",
    "    \n",
    "    def save_history(self):\n",
    "        \"\"\"Guarda la historia completa\"\"\"\n",
    "        history_path = self.log_dir / 'training_history.json'\n",
    "        with open(history_path, 'w') as f:\n",
    "            json.dump(self.history, f, indent=2)\n",
    "        \n",
    "        print(f\"üìÅ Historia guardada en: {history_path}\")\n",
    "        print(f\"üìä M√©tricas CSV en: {self.csv_path}\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Cierra el logger\"\"\"\n",
    "        self.save_history()\n",
    "\n",
    "print(\"‚úÖ Logger implementado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingCallback(BaseCallback):\n",
    "    \"\"\"Callback personalizado para evaluaci√≥n y guardado durante el entrenamiento\"\"\"\n",
    "    \n",
    "    def __init__(self, eval_freq, save_freq, eval_episodes=5, save_path='models/sac_shadow_hand',\n",
    "                 verbose=1, custom_logger=None):\n",
    "        super(TrainingCallback, self).__init__(verbose)\n",
    "        self.eval_freq = eval_freq\n",
    "        self.save_freq = save_freq\n",
    "        self.eval_episodes = eval_episodes\n",
    "        self.save_path = Path(save_path)\n",
    "        self.save_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.custom_logger = custom_logger\n",
    "        \n",
    "        # M√©tricas de seguimiento\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.episode_count = 0\n",
    "        self.last_eval_timestep = 0\n",
    "        self.last_save_timestep = 0\n",
    "        \n",
    "        # Crear entorno de evaluaci√≥n\n",
    "        self.eval_env = None\n",
    "    \n",
    "    def _init_callback(self) -> None:\n",
    "        \"\"\"Inicializa el callback\"\"\"\n",
    "        # Crear entorno de evaluaci√≥n\n",
    "        if self.eval_env is None:\n",
    "            self.eval_env = ShadowHandReachEnvironment(\n",
    "                render_mode=None,\n",
    "                seed=SEED + 1000,  # Semilla diferente para evaluaci√≥n\n",
    "                max_episode_steps=50,\n",
    "                reward_type=\"dense\"\n",
    "            )\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"Ejecutado en cada paso de entrenamiento\"\"\"\n",
    "        # Evaluaci√≥n peri√≥dica\n",
    "        if (self.num_timesteps - self.last_eval_timestep) >= self.eval_freq:\n",
    "            self._evaluate_model()\n",
    "            self.last_eval_timestep = self.num_timesteps\n",
    "        \n",
    "        # Guardado peri√≥dico\n",
    "        if (self.num_timesteps - self.last_save_timestep) >= self.save_freq:\n",
    "            self._save_model()\n",
    "            self.last_save_timestep = self.num_timesteps\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _evaluate_model(self):\n",
    "        \"\"\"Eval√∫a el modelo actual\"\"\"\n",
    "        if self.eval_env is None:\n",
    "            return\n",
    "        \n",
    "        eval_rewards = []\n",
    "        eval_success_rates = []\n",
    "        eval_distances = []\n",
    "        \n",
    "        for _ in range(self.eval_episodes):\n",
    "            obs, _ = self.eval_env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "            \n",
    "            while not done and episode_steps < 50:\n",
    "                action, _ = self.model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = self.eval_env.step(action)\n",
    "                episode_reward += reward\n",
    "                episode_steps += 1\n",
    "                done = terminated or truncated\n",
    "            \n",
    "            eval_rewards.append(episode_reward)\n",
    "            eval_success_rates.append(self.eval_env.get_success_rate(obs))\n",
    "            eval_distances.append(self.eval_env.get_goal_distance(obs))\n",
    "        \n",
    "        mean_reward = np.mean(eval_rewards)\n",
    "        mean_success = np.mean(eval_success_rates)\n",
    "        mean_distance = np.mean(eval_distances)\n",
    "        \n",
    "        # Log de evaluaci√≥n\n",
    "        if self.verbose > 0:\n",
    "            print(f\"\\nüîç Evaluaci√≥n en timestep {self.num_timesteps:,}:\")\n",
    "            print(f\"  üìä Recompensa media: {mean_reward:.2f} ¬± {np.std(eval_rewards):.2f}\")\n",
    "            print(f\"  üéØ Tasa de √©xito media: {mean_success:.3f}\")\n",
    "            print(f\"  üìè Distancia media: {mean_distance:.4f}\")\n",
    "        \n",
    "        # Guardar mejor modelo\n",
    "        if mean_reward > self.best_mean_reward:\n",
    "            self.best_mean_reward = mean_reward\n",
    "            best_model_path = self.save_path / \"sac_best.zip\"\n",
    "            self.model.save(best_model_path)\n",
    "            if self.verbose > 0:\n",
    "                print(f\"  üèÜ ¬°Nuevo mejor modelo guardado! Recompensa: {mean_reward:.2f}\")\n",
    "        \n",
    "        # Log personalizado\n",
    "        if self.custom_logger:\n",
    "            self.custom_logger.log_episode(\n",
    "                timestep=self.num_timesteps,\n",
    "                episode=self.episode_count,\n",
    "                reward=mean_reward,\n",
    "                episode_length=episode_steps,\n",
    "                success_rate=mean_success,\n",
    "                distance=mean_distance,\n",
    "                eval_reward=mean_reward,\n",
    "                eval_success_rate=mean_success\n",
    "            )\n",
    "        \n",
    "        self.episode_count += 1\n",
    "    \n",
    "    def _save_model(self):\n",
    "        \"\"\"Guarda el modelo actual\"\"\"\n",
    "        checkpoint_path = self.save_path / f\"sac_checkpoint_{self.num_timesteps}.zip\"\n",
    "        self.model.save(checkpoint_path)\n",
    "        if self.verbose > 0:\n",
    "            print(f\"üíæ Checkpoint guardado en timestep {self.num_timesteps:,}\")\n",
    "\n",
    "print(\"‚úÖ TrainingCallback implementado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SB3SACAgent:\n",
    "    \"\"\"Agente SAC usando Stable Baselines3 con entrenamiento paralelo\"\"\"\n",
    "    \n",
    "    def __init__(self, env, model_dir='models/sac_shadow_hand', **sac_params):\n",
    "        self.env_factory = env\n",
    "        self.model_dir = Path(model_dir)\n",
    "        self.model_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Extraer par√°metros espec√≠ficos de SAC\n",
    "        self.n_envs = sac_params.pop('n_envs', 4)\n",
    "        self.vec_env_cls = sac_params.pop('vec_env_cls', SubprocVecEnv)\n",
    "        \n",
    "        # Crear entornos vectorizados\n",
    "        self.env = make_vec_env(\n",
    "            env_id=self._make_env_wrapper,\n",
    "            n_envs=self.n_envs,\n",
    "            vec_env_cls=self.vec_env_cls,\n",
    "            seed=SEED\n",
    "        )\n",
    "        \n",
    "        # Crear modelo SAC\n",
    "        self.model = SAC(\n",
    "            policy=\"MultiInputPolicy\",  # Para espacios de observaci√≥n dict\n",
    "            env=self.env,\n",
    "            **sac_params\n",
    "        )\n",
    "        \n",
    "        print(f\"ü§ñ Agente SAC creado con {self.n_envs} entornos paralelos\")\n",
    "        print(f\"‚ö° Vectorizaci√≥n: {self.vec_env_cls.__name__}\")\n",
    "        print(f\"üñ•Ô∏è Dispositivo: {self.model.device}\")\n",
    "    \n",
    "    def _make_env_wrapper(self):\n",
    "        \"\"\"Wrapper para crear entornos individuales\"\"\"\n",
    "        base_env = self.env_factory()\n",
    "        return SuccessInfoWrapper(base_env)\n",
    "    \n",
    "    def train(self, total_timesteps, callback=None, progress_bar=True):\n",
    "        \"\"\"Entrena el agente SAC\"\"\"\n",
    "        print(f\"üöÄ Iniciando entrenamiento por {total_timesteps:,} timesteps...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.model.learn(\n",
    "            total_timesteps=total_timesteps,\n",
    "            callback=callback,\n",
    "            progress_bar=progress_bar\n",
    "        )\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        print(f\"‚úÖ Entrenamiento completado en {training_time/3600:.2f} horas\")\n",
    "        print(f\"‚ö° Velocidad: {total_timesteps/(training_time/3600):,.0f} timesteps/hora\")\n",
    "    \n",
    "    def save(self, filename=None):\n",
    "        \"\"\"Guarda el modelo\"\"\"\n",
    "        if filename is None:\n",
    "            filename = \"sac_final.zip\"\n",
    "        filepath = self.model_dir / filename\n",
    "        self.model.save(filepath)\n",
    "        print(f\"üíæ Modelo guardado en: {filepath}\")\n",
    "    \n",
    "    def load(self, filename=None):\n",
    "        \"\"\"Carga un modelo existente\"\"\"\n",
    "        if filename is None:\n",
    "            # Buscar el mejor modelo disponible\n",
    "            best_path = self.model_dir / \"sac_best.zip\"\n",
    "            final_path = self.model_dir / \"sac_final.zip\"\n",
    "            \n",
    "            if best_path.exists():\n",
    "                filepath = best_path\n",
    "            elif final_path.exists():\n",
    "                filepath = final_path\n",
    "            else:\n",
    "                print(\"‚ùå No se encontraron modelos guardados\")\n",
    "                return False\n",
    "        else:\n",
    "            filepath = self.model_dir / filename\n",
    "        \n",
    "        if not filepath.exists():\n",
    "            print(f\"‚ùå Modelo no encontrado: {filepath}\")\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            self.model = SAC.load(filepath, env=self.env)\n",
    "            print(f\"‚úÖ Modelo cargado desde: {filepath}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error cargando modelo: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def predict(self, observation, deterministic=True):\n",
    "        \"\"\"Predice una acci√≥n para una observaci√≥n dada\"\"\"\n",
    "        return self.model.predict(observation, deterministic=deterministic)\n",
    "    \n",
    "    def evaluate(self, n_episodes=10, render=False):\n",
    "        \"\"\"Eval√∫a el agente entrenado\"\"\"\n",
    "        # Crear entorno de evaluaci√≥n\n",
    "        eval_env = ShadowHandReachEnvironment(\n",
    "            render_mode=\"human\" if render else None,\n",
    "            seed=SEED + 2000,\n",
    "            max_episode_steps=50,\n",
    "            reward_type=\"dense\"\n",
    "        )\n",
    "        \n",
    "        episode_rewards = []\n",
    "        episode_successes = []\n",
    "        episode_distances = []\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            obs, _ = eval_env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done and episode_steps < 50:\n",
    "                action, _ = self.model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "                episode_reward += reward\n",
    "                episode_steps += 1\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                if render:\n",
    "                    eval_env.render()\n",
    "                    time.sleep(0.01)  # Peque√±a pausa para visualizaci√≥n\n",
    "            \n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_successes.append(eval_env.get_success_rate(obs))\n",
    "            episode_distances.append(eval_env.get_goal_distance(obs))\n",
    "            \n",
    "            print(f\"Episodio {episode + 1:2d}: Recompensa = {episode_reward:7.1f}, \"\n",
    "                  f\"√âxito = {eval_env.get_success_rate(obs):.3f}, \"\n",
    "                  f\"Distancia = {eval_env.get_goal_distance(obs):.4f}, Pasos = {episode_steps}\")\n",
    "        \n",
    "        eval_env.close()\n",
    "        \n",
    "        # Calcular estad√≠sticas\n",
    "        mean_reward = np.mean(episode_rewards)\n",
    "        std_reward = np.std(episode_rewards)\n",
    "        mean_success = np.mean(episode_successes)\n",
    "        mean_distance = np.mean(episode_distances)\n",
    "        \n",
    "        print(f\"\\nüìä Resultados de Evaluaci√≥n ({n_episodes} episodios):\")\n",
    "        print(f\"  üìà Recompensa promedio: {mean_reward:.2f} ¬± {std_reward:.2f}\")\n",
    "        print(f\"  üéØ Tasa de √©xito promedio: {mean_success:.3f}\")\n",
    "        print(f\"  üìè Distancia promedio: {mean_distance:.4f}\")\n",
    "        print(f\"  üìä Recompensa m√°xima: {max(episode_rewards):.2f}\")\n",
    "        print(f\"  üìä Recompensa m√≠nima: {min(episode_rewards):.2f}\")\n",
    "        \n",
    "        return {\n",
    "            'mean_reward': mean_reward,\n",
    "            'std_reward': std_reward,\n",
    "            'mean_success': mean_success,\n",
    "            'mean_distance': mean_distance,\n",
    "            'all_rewards': episode_rewards,\n",
    "            'all_successes': episode_successes,\n",
    "            'all_distances': episode_distances\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ SB3SACAgent implementado correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Configuraci√≥n de Entrenamiento\n",
    "\n",
    "### 4.1 Hiperpar√°metros Optimizados para SAC\n",
    "\n",
    "Los hiperpar√°metros han sido cuidadosamente seleccionados para la tarea de manipulaci√≥n rob√≥tica Shadow Hand Reach:\n",
    "\n",
    "#### 4.1.1 Par√°metros de Red\n",
    "‚Ä¢ **Arquitectura de red:** 512√ó512√ó256 para actor y cr√≠tico\n",
    "‚Ä¢ **Funci√≥n de activaci√≥n:** ReLU\n",
    "‚Ä¢ **Learning rate:** 3e-4 (est√°ndar para tareas rob√≥ticas)\n",
    "\n",
    "#### 4.1.2 Par√°metros de Aprendizaje\n",
    "‚Ä¢ **Tama√±o de buffer:** 1,000,000 (grande para retener experiencias diversas)\n",
    "‚Ä¢ **Batch size:** 256 (balance entre estabilidad y eficiencia)\n",
    "‚Ä¢ **Gamma (descuento):** 0.99 (horizonte largo para manipulaci√≥n)\n",
    "‚Ä¢ **Tau (soft update):** 0.005 (actualizaciones suaves)\n",
    "\n",
    "#### 4.1.3 Par√°metros de Exploraci√≥n\n",
    "‚Ä¢ **Coeficiente de entrop√≠a:** Auto (SAC ajusta autom√°ticamente)\n",
    "‚Ä¢ **Target entropy:** Auto (basado en dimensi√≥n de acci√≥n)\n",
    "\n",
    "#### 4.1.4 Entrenamiento Paralelo\n",
    "‚Ä¢ **N√∫mero de entornos:** 4 (balance entre paralelizaci√≥n y recursos)\n",
    "‚Ä¢ **Vectorizaci√≥n:** SubprocVecEnv (verdadero paralelismo)\n",
    "‚Ä¢ **Learning starts:** 10,000 (acumular experiencias antes de entrenar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sac_parallel(\n",
    "    episodes=5000,\n",
    "    n_envs=4,\n",
    "    eval_interval=1000,  # Convertido a timesteps en el callback\n",
    "    save_interval=2000,  # Convertido a timesteps en el callback\n",
    "    render=False,\n",
    "    restart=False,\n",
    "    reward_type='dense',\n",
    "    max_episode_steps=50,\n",
    "    vec_env_cls=SubprocVecEnv\n",
    "):\n",
    "    \"\"\"\n",
    "    Entrena un agente SAC con entornos paralelos en Shadow Dexterous Hand Reach.\n",
    "    \n",
    "    Args:\n",
    "        episodes: N√∫mero de episodios de entrenamiento\n",
    "        n_envs: N√∫mero de entornos paralelos\n",
    "        eval_interval: Episodios entre evaluaciones\n",
    "        save_interval: Episodios entre checkpoints\n",
    "        render: Habilitar renderizado durante entrenamiento (solo n_envs=1)\n",
    "        restart: Cargar y continuar desde el mejor modelo guardado\n",
    "        reward_type: Tipo de funci√≥n de recompensa ('dense' o 'sparse')\n",
    "        max_episode_steps: M√°ximo de pasos por episodio\n",
    "        vec_env_cls: Clase de entorno vectorizado\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üöÄ INICIANDO ENTRENAMIENTO SAC PARALELO EN SHADOW DEXTEROUS HAND REACH\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Deshabilitar renderizado para entrenamiento paralelo\n",
    "    if n_envs > 1 and render:\n",
    "        print(\"‚ö†Ô∏è Advertencia: Renderizado deshabilitado para entrenamiento paralelo (n_envs > 1)\")\n",
    "        render = False\n",
    "    \n",
    "    # Factory function para crear entornos\n",
    "    def env_factory():\n",
    "        return ShadowHandReachEnvironment(\n",
    "            render_mode=\"human\" if render and n_envs == 1 else None,\n",
    "            seed=SEED,\n",
    "            max_episode_steps=max_episode_steps,\n",
    "            reward_type=reward_type\n",
    "        )\n",
    "    \n",
    "    # Obtener informaci√≥n del entorno desde una instancia √∫nica\n",
    "    single_env = env_factory()\n",
    "    state_dim = single_env.get_state_dim()\n",
    "    action_dim = single_env.get_action_dim()\n",
    "    action_bounds = single_env.get_action_bounds()\n",
    "    single_env.close()\n",
    "    \n",
    "    print(f\"üìã Informaci√≥n del entorno:\")\n",
    "    print(f\"  üìè Dimensi√≥n del estado: {state_dim}\")\n",
    "    print(f\"  üéÆ Dimensi√≥n de la acci√≥n: {action_dim}\")\n",
    "    print(f\"  üìä L√≠mites de acci√≥n: [{action_bounds[0][0]:.1f}, {action_bounds[1][0]:.1f}]\")\n",
    "    print(f\"  üîÑ Entornos paralelos: {n_envs}\")\n",
    "    print(f\"  ‚ö° Vectorizaci√≥n: {vec_env_cls.__name__}\")\n",
    "    \n",
    "    # Hiperpar√°metros SAC optimizados para Shadow Hand Reach\n",
    "    sac_params = {\n",
    "        'learning_rate': 3e-4,      # Est√°ndar para tareas rob√≥ticas\n",
    "        'buffer_size': 1_000_000,\n",
    "        'learning_starts': 10000,   # Empezar a aprender despu√©s de colectar datos\n",
    "        'batch_size': 256,\n",
    "        'tau': 0.005,               # Tasa de actualizaci√≥n suave\n",
    "        'gamma': 0.99,\n",
    "        'train_freq': 1,\n",
    "        'gradient_steps': 1,\n",
    "        'ent_coef': 'auto',         # Ajuste autom√°tico de entrop√≠a\n",
    "        'target_update_interval': 1,\n",
    "        'target_entropy': 'auto',\n",
    "        'use_sde': False,\n",
    "        'policy_kwargs': dict(\n",
    "            net_arch=dict(pi=[512, 512, 256], qf=[512, 512, 256]),  # Redes m√°s grandes para tarea compleja\n",
    "            activation_fn=torch.nn.ReLU\n",
    "        ),\n",
    "        'verbose': 1,\n",
    "        'seed': SEED,\n",
    "        'tensorboard_log': 'runs/sac_shadow_hand',\n",
    "        'n_envs': n_envs,\n",
    "        'vec_env_cls': vec_env_cls\n",
    "    }\n",
    "    \n",
    "    # Inicializar agente SAC con entornos paralelos\n",
    "    agent = SB3SACAgent(\n",
    "        env=env_factory,\n",
    "        model_dir='models/sac_shadow_hand',\n",
    "        **sac_params\n",
    "    )\n",
    "    \n",
    "    # Cargar modelos existentes si se especifica restart\n",
    "    if restart:\n",
    "        print(\"üîÑ Cargando modelos existentes...\")\n",
    "        if agent.load():\n",
    "            print(\"‚úÖ Modelos cargados exitosamente\")\n",
    "        else:\n",
    "            print(\"‚ùå No se encontraron modelos, iniciando desde cero\")\n",
    "    \n",
    "    print(f\"üñ•Ô∏è Dispositivo: {agent.model.device}\")\n",
    "    print(f\"üóÉÔ∏è Tama√±o del buffer: {agent.model.buffer_size:,}\")\n",
    "    print(f\"üì¶ Tama√±o del batch: {agent.model.batch_size}\")\n",
    "    print(f\"üìà Pasos de gradiente por actualizaci√≥n: {agent.model.gradient_steps}\")\n",
    "    \n",
    "    # Calcular timesteps totales\n",
    "    total_timesteps = episodes * max_episode_steps * n_envs\n",
    "    print(f\"‚è±Ô∏è Timesteps totales a entrenar: {total_timesteps:,}\")\n",
    "    print(f\"üîç Evaluaci√≥n cada: {eval_interval} episodios\")\n",
    "    print(f\"üíæ Guardar checkpoint cada: {save_interval} episodios\")\n",
    "    print(f\"üéØ Tipo de recompensa: {reward_type}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Inicializar logger para logging CSV\n",
    "    logger = Logger(log_dir='logs/sac_shadow_hand', tensorboard_dir='runs/sac_shadow_hand')\n",
    "    \n",
    "    # Configurar callback de entrenamiento\n",
    "    callback = TrainingCallback(\n",
    "        eval_freq=eval_interval * max_episode_steps,  # Convertir episodios a timesteps\n",
    "        save_freq=save_interval * max_episode_steps,\n",
    "        eval_episodes=5,\n",
    "        save_path='models/sac_shadow_hand',\n",
    "        verbose=1,\n",
    "        custom_logger=logger\n",
    "    )\n",
    "    \n",
    "    print(f\"üöÄ Iniciando entrenamiento paralelo...\")\n",
    "    print(f\"üìä Episodios esperados por entorno: ~{episodes}\")\n",
    "    print(f\"üìà Total de episodios esperados en todos los entornos: ~{episodes * n_envs}\")\n",
    "    print(f\"‚è±Ô∏è Entrenando por {total_timesteps:,} timesteps totales\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Entrenar el agente\n",
    "    agent.train(\n",
    "        total_timesteps=total_timesteps,\n",
    "        callback=callback,\n",
    "        progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Guardado final\n",
    "    print(\"üíæ Guardando modelos finales...\")\n",
    "    agent.save()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(\"=\" * 80)\n",
    "    print(\"üéâ ¬°ENTRENAMIENTO PARALELO COMPLETADO!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üîÑ Entornos de entrenamiento: {n_envs}\")\n",
    "    print(f\"‚ö° Vectorizaci√≥n: {vec_env_cls.__name__}\")\n",
    "    print(f\"‚è∞ Tiempo total de entrenamiento: {total_time/3600:.2f} horas\")\n",
    "    print(f\"‚è±Ô∏è Timesteps totales: {total_timesteps:,}\")\n",
    "    print(f\"üöÄ Timesteps por hora: {total_timesteps/(total_time/3600):,.0f}\")\n",
    "    print(f\"üìà Aceleraci√≥n vs. entorno √∫nico: ~{n_envs}x (te√≥rica)\")\n",
    "    \n",
    "    # Guardar informaci√≥n del entrenamiento\n",
    "    training_info = {\n",
    "        'n_envs': n_envs,\n",
    "        'vec_env_cls': vec_env_cls.__name__,\n",
    "        'total_timesteps': total_timesteps,\n",
    "        'training_time_hours': total_time / 3600,\n",
    "        'timesteps_per_hour': total_timesteps / (total_time / 3600),\n",
    "        'theoretical_speedup': n_envs,\n",
    "        'sac_params': sac_params,\n",
    "        'reward_type': reward_type,\n",
    "        'max_episode_steps': max_episode_steps,\n",
    "        'environment': 'Shadow Dexterous Hand Reach (Dense)',\n",
    "        'completion_timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    info_path = Path('models/sac_shadow_hand/training_info.json')\n",
    "    info_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(info_path, 'w') as f:\n",
    "        json.dump(training_info, f, indent=2)\n",
    "    \n",
    "    # Guardar historia del logger\n",
    "    logger.close()\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Limpiar recursos\n",
    "    agent.env.close()\n",
    "    \n",
    "    return agent\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de entrenamiento definida correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Entrenamiento del Agente\n",
    "\n",
    "### 5.1 Par√°metros de Entrenamiento\n",
    "\n",
    "‚Ä¢ **Episodios:** 5000 por entorno (20,000 episodios totales con 4 entornos)\n",
    "‚Ä¢ **Timesteps totales:** ~1,000,000 (5000 √ó 50 √ó 4)\n",
    "‚Ä¢ **Entornos paralelos:** 4 (para acelerar el entrenamiento)\n",
    "‚Ä¢ **Evaluaci√≥n:** Cada 1000 episodios\n",
    "‚Ä¢ **Guardado:** Cada 2000 episodios\n",
    "\n",
    "#### 5.1.1 Estimaci√≥n de Tiempo\n",
    "\n",
    "Con 4 entornos paralelos, esperamos una aceleraci√≥n de ~3-4x comparado con entrenamiento secuencial. El entrenamiento completo deber√≠a tomar aproximadamente 4-8 horas dependiendo del hardware.\n",
    "\n",
    "**Nota:** Para este notebook de demostraci√≥n, usaremos menos episodios. Para entrenamiento completo, aumentar a 5000+ episodios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar entrenamiento con par√°metros reducidos para demostraci√≥n\n",
    "# Para entrenamiento completo, usar episodes=5000 o m√°s\n",
    "trained_agent = train_sac_parallel(\n",
    "    episodes=1000,  # Reducido para demostraci√≥n - usar 5000+ para entrenamiento completo\n",
    "    n_envs=4,       # 4 entornos paralelos\n",
    "    eval_interval=100,   # Evaluar cada 100 episodios\n",
    "    save_interval=500,   # Guardar cada 500 episodios\n",
    "    render=False,        # Sin renderizado durante entrenamiento\n",
    "    restart=False,       # Iniciar desde cero (cambiar a True para continuar entrenamiento)\n",
    "    reward_type='dense',\n",
    "    max_episode_steps=50,\n",
    "    vec_env_cls=SubprocVecEnv\n",
    ")\n",
    "\n",
    "print(\"\\nüéâ ¬°Entrenamiento completado exitosamente!\")\n",
    "print(\"üìÅ Modelos guardados en: models/sac_shadow_hand/\")\n",
    "print(\"üìä Logs guardados en: logs/sac_shadow_hand/\")\n",
    "print(\"üìà TensorBoard logs en: runs/sac_shadow_hand/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Evaluaci√≥n del Agente Entrenado\n",
    "\n",
    "Ahora evaluaremos el rendimiento del agente entrenado ejecutando m√∫ltiples episodios y analizando las m√©tricas de rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trained_agent(model_path='models/sac_shadow_hand/sac_best.zip', \n",
    "                          episodes=10, render=False):\n",
    "    \"\"\"Eval√∫a un agente SAC entrenado\"\"\"\n",
    "    print(\"üîç Evaluando agente entrenado...\")\n",
    "    \n",
    "    # Crear entorno de evaluaci√≥n\n",
    "    eval_env = ShadowHandReachEnvironment(\n",
    "        render_mode=\"human\" if render else None,\n",
    "        seed=SEED + 3000,  # Semilla diferente para evaluaci√≥n\n",
    "        max_episode_steps=50,\n",
    "        reward_type=\"dense\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Cargar el modelo entrenado\n",
    "        model = SAC.load(model_path, env=SuccessInfoWrapper(eval_env))\n",
    "        \n",
    "        total_rewards = []\n",
    "        total_successes = []\n",
    "        total_distances = []\n",
    "        episode_lengths = []\n",
    "        \n",
    "        print(f\"\\nüéØ Ejecutando {episodes} episodios de evaluaci√≥n...\")\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            obs, _ = eval_env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done and episode_steps < 50:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "                episode_reward += reward\n",
    "                episode_steps += 1\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                if render:\n",
    "                    eval_env.render()\n",
    "                    time.sleep(0.01)\n",
    "            \n",
    "            success_rate = eval_env.get_success_rate(obs)\n",
    "            distance = eval_env.get_goal_distance(obs)\n",
    "            \n",
    "            total_rewards.append(episode_reward)\n",
    "            total_successes.append(success_rate)\n",
    "            total_distances.append(distance)\n",
    "            episode_lengths.append(episode_steps)\n",
    "            \n",
    "            print(f\"Episodio {i+1:2d}: Recompensa = {episode_reward:7.1f}, \"\n",
    "                  f\"√âxito = {success_rate:.3f}, Distancia = {distance:.4f}, Pasos = {episode_steps}\")\n",
    "        \n",
    "        # Calcular estad√≠sticas\n",
    "        avg_reward = np.mean(total_rewards)\n",
    "        std_reward = np.std(total_rewards)\n",
    "        avg_success = np.mean(total_successes)\n",
    "        avg_distance = np.mean(total_distances)\n",
    "        avg_length = np.mean(episode_lengths)\n",
    "        \n",
    "        # Calcular tasa de √©xito binaria (episodios con success > 0.5)\n",
    "        success_rate_binary = np.sum(np.array(total_successes) > 0.5) / episodes * 100\n",
    "        \n",
    "        print(f\"\\nüìä Resultados de Evaluaci√≥n ({episodes} episodios):\")\n",
    "        print(f\"  üìà Recompensa promedio: {avg_reward:.2f} ¬± {std_reward:.2f}\")\n",
    "        print(f\"  üìä Recompensa m√°xima: {max(total_rewards):.2f}\")\n",
    "        print(f\"  üìä Recompensa m√≠nima: {min(total_rewards):.2f}\")\n",
    "        print(f\"  üéØ √âxito promedio: {avg_success:.3f}\")\n",
    "        print(f\"  ‚úÖ Tasa de √©xito (>0.5): {success_rate_binary:.1f}%\")\n",
    "        print(f\"  üìè Distancia promedio: {avg_distance:.4f}\")\n",
    "        print(f\"  ‚è±Ô∏è Duraci√≥n promedio: {avg_length:.1f} pasos\")\n",
    "        \n",
    "        return {\n",
    "            'rewards': total_rewards,\n",
    "            'successes': total_successes,\n",
    "            'distances': total_distances,\n",
    "            'lengths': episode_lengths,\n",
    "            'avg_reward': avg_reward,\n",
    "            'std_reward': std_reward,\n",
    "            'avg_success': avg_success,\n",
    "            'avg_distance': avg_distance,\n",
    "            'success_rate_binary': success_rate_binary,\n",
    "            'avg_length': avg_length\n",
    "        }\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Modelo no encontrado en: {model_path}\")\n",
    "        print(\"üîß Aseg√∫rate de haber ejecutado el entrenamiento primero.\")\n",
    "        return None\n",
    "    \n",
    "    finally:\n",
    "        eval_env.close()\n",
    "\n",
    "# Evaluar el agente entrenado\n",
    "eval_results = evaluate_trained_agent(\n",
    "    episodes=20,  # Aumentar para evaluaci√≥n m√°s completa\n",
    "    render=False  # Cambiar a True para visualizar el agente\n",
    ")\n",
    "\n",
    "if eval_results:\n",
    "    print(\"\\n‚úÖ Evaluaci√≥n completada exitosamente\")\n",
    "else:\n",
    "    print(\"‚ùå Evaluaci√≥n fall√≥ - verifica que el modelo est√© entrenado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 An√°lisis de Resultados\n",
    "\n",
    "Para este an√°lisis de resultados de Shadow Dexterous Hand Reach utilizaremos las siguientes m√©tricas clave:\n",
    "\n",
    "1. **Recompensa por timestep** (tendencia de aprendizaje)\n",
    "2. **Tasa de √©xito** (qu√© tan bien alcanza los objetivos)\n",
    "3. **Distancia al objetivo** (precisi√≥n del control)\n",
    "4. **Duraci√≥n de episodios** (eficiencia del agente)\n",
    "5. **Recompensas de evaluaci√≥n** (rendimiento en episodios deterministas)\n",
    "\n",
    "### 7.1 ¬øQu√© nos dice cada m√©trica?\n",
    "\n",
    "#### 7.1.1 Recompensa por Timestep\n",
    "En tareas de manipulaci√≥n rob√≥tica como Shadow Hand Reach, la recompensa por timestep indica:\n",
    "- **Tendencia creciente:** El agente aprende a coordinar m√∫ltiples dedos m√°s eficientemente\n",
    "- **Estabilizaci√≥n:** El agente ha convergido a una pol√≠tica consistente\n",
    "- **Variabilidad:** Natural en tareas complejas debido a aleatoriedad en posiciones objetivo\n",
    "\n",
    "#### 7.1.2 Tasa de √âxito\n",
    "La m√©trica m√°s importante para evaluar el rendimiento:\n",
    "- **Valores cercanos a 1.0:** Agente logra alcanzar objetivos consistentemente con todas las puntas de dedos\n",
    "- **Mejora gradual:** Indica aprendizaje progresivo de coordinaci√≥n entre m√∫ltiples articulaciones\n",
    "- **Plateau alto:** Convergencia exitosa de la pol√≠tica\n",
    "\n",
    "#### 7.1.3 Distancia al Objetivo\n",
    "- **Distancia decreciente:** Mejora en la precisi√≥n del control\n",
    "- **Convergencia a valores bajos:** Indica control preciso de las puntas de dedos\n",
    "- **Consistencia:** Pol√≠tica robusta y determinista\n",
    "\n",
    "#### 7.1.4 Duraci√≥n de Episodios\n",
    "‚Ä¢ **Episodios m√°s largos inicialmente:** Exploraci√≥n y aprendizaje\n",
    "‚Ä¢ **Estabilizaci√≥n:** Agente desarrolla estrategia eficiente\n",
    "‚Ä¢ **Consistencia:** Indica pol√≠tica robusta y determinista\n",
    "\n",
    "#### 7.1.5 Recompensas de Evaluaci√≥n\n",
    "‚Ä¢ **Evaluaci√≥n determinista:** Sin exploraci√≥n, solo explotaci√≥n de la pol√≠tica aprendida\n",
    "‚Ä¢ **Consistencia alta:** Indica aprendizaje robusto\n",
    "‚Ä¢ **Mejora continua:** Pol√≠tica sigue optimiz√°ndose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar y analizar datos de entrenamiento\n",
    "try:\n",
    "    # Cargar datos del CSV de entrenamiento\n",
    "    training_data = pd.read_csv('logs/sac_shadow_hand/training_log.csv')\n",
    "    print(f\"üìä Datos de entrenamiento cargados: {len(training_data)} registros\")\n",
    "    \n",
    "    # Verificar columnas disponibles\n",
    "    print(f\"üìã Columnas disponibles: {list(training_data.columns)}\")\n",
    "    \n",
    "    # Calcular promedios m√≥viles para suavizar las curvas\n",
    "    window_size = min(50, len(training_data) // 10)  # Ventana adaptativa\n",
    "    if window_size > 1:\n",
    "        training_data['reward_ma'] = training_data['reward'].rolling(window=window_size, min_periods=1).mean()\n",
    "        training_data['success_ma'] = training_data['success_rate'].rolling(window=window_size, min_periods=1).mean()\n",
    "        training_data['distance_ma'] = training_data['distance'].rolling(window=window_size, min_periods=1).mean()\n",
    "        training_data['eval_reward_ma'] = training_data['eval_reward'].rolling(window=window_size, min_periods=1).mean()\n",
    "    \n",
    "    # Crear visualizaci√≥n completa de m√©tricas de entrenamiento\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('M√©tricas de Entrenamiento SAC - Shadow Dexterous Hand Reach', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Recompensas de Entrenamiento\n",
    "    axes[0,0].plot(training_data['timestep'], training_data['reward'], alpha=0.3, color='lightblue', label='Recompensa por Episodio')\n",
    "    if window_size > 1:\n",
    "        axes[0,0].plot(training_data['timestep'], training_data['reward_ma'], color='blue', linewidth=2, label=f'Promedio M√≥vil ({window_size})')\n",
    "    axes[0,0].axhline(y=0, color='red', linestyle='--', alpha=0.7, label='L√≠nea Base (0)')\n",
    "    axes[0,0].set_xlabel('Timesteps')\n",
    "    axes[0,0].set_ylabel('Recompensa')\n",
    "    axes[0,0].set_title('Recompensas de Entrenamiento a lo Largo del Tiempo')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Tasa de √âxito\n",
    "    axes[0,1].plot(training_data['timestep'], training_data['success_rate'], alpha=0.4, color='green', label='Tasa de √âxito')\n",
    "    if window_size > 1:\n",
    "        axes[0,1].plot(training_data['timestep'], training_data['success_ma'], color='darkgreen', linewidth=2, label=f'Promedio M√≥vil ({window_size})')\n",
    "    axes[0,1].axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='Umbral de √âxito (0.5)')\n",
    "    axes[0,1].axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Buen Rendimiento (0.8)')\n",
    "    axes[0,1].set_xlabel('Timesteps')\n",
    "    axes[0,1].set_ylabel('Tasa de √âxito')\n",
    "    axes[0,1].set_title('Tasa de √âxito Durante el Entrenamiento')\n",
    "    axes[0,1].set_ylim(0, 1)\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Distancia al Objetivo\n",
    "    axes[1,0].plot(training_data['timestep'], training_data['distance'], alpha=0.4, color='purple', label='Distancia al Objetivo')\n",
    "    if window_size > 1:\n",
    "        axes[1,0].plot(training_data['timestep'], training_data['distance_ma'], color='darkviolet', linewidth=2, label=f'Promedio M√≥vil ({window_size})')\n",
    "    axes[1,0].axhline(y=0.05, color='red', linestyle='--', alpha=0.7, label='Umbral de √âxito (0.05)')\n",
    "    axes[1,0].set_xlabel('Timesteps')\n",
    "    axes[1,0].set_ylabel('Distancia al Objetivo')\n",
    "    axes[1,0].set_title('Distancia al Objetivo a lo Largo del Tiempo')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Recompensas de Evaluaci√≥n\n",
    "    eval_data = training_data[training_data['eval_reward'] > 0]  # Solo mostrar puntos con evaluaci√≥n\n",
    "    if len(eval_data) > 0:\n",
    "        axes[1,1].scatter(eval_data['timestep'], eval_data['eval_reward'], color='orange', alpha=0.7, label='Recompensa de Evaluaci√≥n')\n",
    "        if len(eval_data) > 2 and window_size > 1:\n",
    "            axes[1,1].plot(eval_data['timestep'], eval_data['eval_reward_ma'], color='darkorange', linewidth=2, label='Tendencia')\n",
    "        axes[1,1].set_xlabel('Timesteps')\n",
    "        axes[1,1].set_ylabel('Recompensa de Evaluaci√≥n')\n",
    "        axes[1,1].set_title('Rendimiento en Evaluaciones Deterministas')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1,1].text(0.5, 0.5, 'No hay datos de evaluaci√≥n\\ndisponibles',\n",
    "                      horizontalalignment='center', verticalalignment='center',\n",
    "                      transform=axes[1,1].transAxes, fontsize=12)\n",
    "        axes[1,1].set_title('Recompensas de Evaluaci√≥n')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Estad√≠sticas finales\n",
    "    final_reward = training_data['reward'].iloc[-10:].mean() if len(training_data) >= 10 else training_data['reward'].mean()\n",
    "    final_success = training_data['success_rate'].iloc[-10:].mean() if len(training_data) >= 10 else training_data['success_rate'].mean()\n",
    "    final_distance = training_data['distance'].iloc[-10:].mean() if len(training_data) >= 10 else training_data['distance'].mean()\n",
    "    \n",
    "    print(f\"\\nüìà Estad√≠sticas Finales de Entrenamiento:\")\n",
    "    print(f\"  üèÜ Recompensa promedio (√∫ltimos 10 episodios): {final_reward:.2f}\")\n",
    "    print(f\"  üéØ Tasa de √©xito promedio (√∫ltimos 10 episodios): {final_success:.3f}\")\n",
    "    print(f\"  üìè Distancia promedio (√∫ltimos 10 episodios): {final_distance:.4f}\")\n",
    "    print(f\"  üìä Total de registros de entrenamiento: {len(training_data)}\")\n",
    "    \n",
    "    if len(eval_data) > 0:\n",
    "        best_eval = eval_data['eval_reward'].max()\n",
    "        print(f\"  ü•á Mejor recompensa de evaluaci√≥n: {best_eval:.2f}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå No se encontraron datos de entrenamiento\")\n",
    "    print(\"üîß Ejecuta el entrenamiento primero para generar los datos\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al cargar datos: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Visualizaci√≥n Detallada del Progreso de Aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis m√°s detallado con m√∫ltiples ventanas de promedio m√≥vil\n",
    "try:\n",
    "    if 'training_data' in locals() and len(training_data) > 0:\n",
    "        # Calcular m√∫ltiples promedios m√≥viles\n",
    "        window_sizes = [10, 25, 50, 100]\n",
    "        colors = ['red', 'orange', 'blue', 'green']\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Subplot 1: Recompensas con m√∫ltiples promedios m√≥viles\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(training_data['timestep'], training_data['reward'], alpha=0.1, color='gray', label='Recompensas Brutas')\n",
    "        for window, color in zip(window_sizes, colors):\n",
    "            if len(training_data) >= window:\n",
    "                smoothed = training_data['reward'].rolling(window=window, min_periods=1).mean()\n",
    "                plt.plot(training_data['timestep'], smoothed, color=color, linewidth=2,\n",
    "                        label=f'Promedio M√≥vil {window}')\n",
    "        plt.axhline(y=0, color='black', linestyle='--', alpha=0.5, label='L√≠nea Base')\n",
    "        plt.xlabel('Timesteps')\n",
    "        plt.ylabel('Recompensa')\n",
    "        plt.title('Curva de Aprendizaje SAC - M√∫ltiples Suavizados')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Subplot 2: Tasa de √©xito vs Distancia\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(training_data['timestep'], training_data['success_rate'], alpha=0.3, color='green', label='Tasa de √âxito')\n",
    "        ax2 = plt.gca().twinx()\n",
    "        ax2.plot(training_data['timestep'], training_data['distance'], alpha=0.3, color='purple', label='Distancia')\n",
    "        \n",
    "        success_smooth = training_data['success_rate'].rolling(window=50, min_periods=1).mean()\n",
    "        distance_smooth = training_data['distance'].rolling(window=50, min_periods=1).mean()\n",
    "        \n",
    "        plt.plot(training_data['timestep'], success_smooth, color='darkgreen', linewidth=3, label='√âxito (Suavizado)')\n",
    "        ax2.plot(training_data['timestep'], distance_smooth, color='darkviolet', linewidth=3, label='Distancia (Suavizada)')\n",
    "        \n",
    "        plt.axhline(y=0.8, color='red', linestyle='--', alpha=0.7, label='Buen Rendimiento')\n",
    "        plt.xlabel('Timesteps')\n",
    "        plt.ylabel('Tasa de √âxito', color='green')\n",
    "        ax2.set_ylabel('Distancia al Objetivo', color='purple')\n",
    "        plt.title('Progreso en √âxito y Precisi√≥n')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend(loc='upper left')\n",
    "        ax2.legend(loc='upper right')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Subplot 3: Distribuci√≥n de recompensas por fase de entrenamiento\n",
    "        plt.subplot(2, 2, 3)\n",
    "        early_rewards = training_data['reward'][:len(training_data)//3]\n",
    "        mid_rewards = training_data['reward'][len(training_data)//3:2*len(training_data)//3]\n",
    "        late_rewards = training_data['reward'][2*len(training_data)//3:]\n",
    "        \n",
    "        plt.hist(early_rewards, alpha=0.5, color='red', label=f'Inicial (n={len(early_rewards)})', bins=20)\n",
    "        plt.hist(mid_rewards, alpha=0.5, color='orange', label=f'Medio (n={len(mid_rewards)})', bins=20)\n",
    "        plt.hist(late_rewards, alpha=0.5, color='green', label=f'Final (n={len(late_rewards)})', bins=20)\n",
    "        plt.xlabel('Recompensa')\n",
    "        plt.ylabel('Frecuencia')\n",
    "        plt.title('Distribuci√≥n de Recompensas por Fase')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Subplot 4: Correlaci√≥n entre distancia y √©xito\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.scatter(training_data['distance'], training_data['success_rate'], alpha=0.5, color='blue')\n",
    "        # A√±adir l√≠nea de tendencia\n",
    "        z = np.polyfit(training_data['distance'], training_data['success_rate'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(training_data['distance'], p(training_data['distance']), \"r--\", alpha=0.8, linewidth=2)\n",
    "        correlation = np.corrcoef(training_data['distance'], training_data['success_rate'])[0,1]\n",
    "        plt.xlabel('Distancia al Objetivo')\n",
    "        plt.ylabel('Tasa de √âxito')\n",
    "        plt.title(f'Correlaci√≥n Distancia-√âxito (r={correlation:.3f})')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # An√°lisis estad√≠stico por fases\n",
    "        print(\"\\nüìä An√°lisis Estad√≠stico por Fases de Entrenamiento:\")\n",
    "        print(f\"\\nüî¥ Fase Inicial (primeros {len(early_rewards)} episodios):\")\n",
    "        print(f\"  üìà Recompensa promedio: {early_rewards.mean():.2f} ¬± {early_rewards.std():.2f}\")\n",
    "        print(f\"  üìä Rango: [{early_rewards.min():.2f}, {early_rewards.max():.2f}]\")\n",
    "        \n",
    "        print(f\"\\nüü† Fase Media ({len(mid_rewards)} episodios):\")\n",
    "        print(f\"  üìà Recompensa promedio: {mid_rewards.mean():.2f} ¬± {mid_rewards.std():.2f}\")\n",
    "        print(f\"  üìä Rango: [{mid_rewards.min():.2f}, {mid_rewards.max():.2f}]\")\n",
    "        \n",
    "        print(f\"\\nüü¢ Fase Final (√∫ltimos {len(late_rewards)} episodios):\")\n",
    "        print(f\"  üìà Recompensa promedio: {late_rewards.mean():.2f} ¬± {late_rewards.std():.2f}\")\n",
    "        print(f\"  üìä Rango: [{late_rewards.min():.2f}, {late_rewards.max():.2f}]\")\n",
    "        \n",
    "        # Mejora total\n",
    "        improvement = late_rewards.mean() - early_rewards.mean()\n",
    "        print(f\"\\nüöÄ Mejora Total: {improvement:.2f} puntos de recompensa\")\n",
    "        print(f\"üìè Correlaci√≥n distancia-√©xito: {correlation:.3f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No hay datos de entrenamiento disponibles para an√°lisis detallado\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error en an√°lisis detallado: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Demostraci√≥n Visual del Agente\n",
    "\n",
    "En esta secci√≥n, ejecutaremos el agente entrenado con visualizaci√≥n para observar su comportamiento en la tarea de alcanzar objetivos con la mano rob√≥tica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostraci√≥n visual del agente entrenado\n",
    "# Nota: Esto requerir√° un entorno con capacidad de renderizado (GUI)\n",
    "def demonstrate_agent(model_path='models/sac_shadow_hand/sac_best.zip', episodes=3):\n",
    "    \"\"\"\n",
    "    Demuestra el agente entrenado con renderizado visual\n",
    "    Nota: Esta funci√≥n requiere un entorno con capacidad de renderizado.\n",
    "    En Google Colab, esto puede no funcionar debido a limitaciones de GUI.\n",
    "    \"\"\"\n",
    "    print(\"üé¨ Iniciando demostraci√≥n visual del agente...\")\n",
    "    print(\"üì∫ Nota: La visualizaci√≥n puede no funcionar en todos los entornos\")\n",
    "    \n",
    "    try:\n",
    "        # Crear entorno con renderizado\n",
    "        demo_env = ShadowHandReachEnvironment(\n",
    "            render_mode=\"human\",  # Renderizado visual\n",
    "            seed=SEED + 4000,\n",
    "            max_episode_steps=50,\n",
    "            reward_type=\"dense\"\n",
    "        )\n",
    "        \n",
    "        # Cargar modelo\n",
    "        model = SAC.load(model_path, env=SuccessInfoWrapper(demo_env))\n",
    "        print(f\"‚úÖ Modelo cargado exitosamente\")\n",
    "        print(f\"üéØ Ejecutando {episodes} episodios de demostraci√≥n...\")\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            print(f\"\\nüé¨ Episodio de demostraci√≥n {episode + 1}/{episodes}\")\n",
    "            obs, _ = demo_env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done and episode_steps < 50:\n",
    "                # Predicci√≥n determinista para demostraci√≥n\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                \n",
    "                # Ejecutar acci√≥n\n",
    "                obs, reward, terminated, truncated, info = demo_env.step(action)\n",
    "                episode_reward += reward\n",
    "                episode_steps += 1\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                # Renderizar\n",
    "                demo_env.render()\n",
    "                time.sleep(0.02)  # Pausa para visualizaci√≥n suave\n",
    "            \n",
    "            success_rate = demo_env.get_success_rate(obs)\n",
    "            distance = demo_env.get_goal_distance(obs)\n",
    "            \n",
    "            print(f\"  üìä Recompensa: {episode_reward:.2f}\")\n",
    "            print(f\"  üéØ Tasa de √©xito: {success_rate:.3f}\")\n",
    "            print(f\"  üìè Distancia final: {distance:.4f}\")\n",
    "            print(f\"  ‚è±Ô∏è Duraci√≥n: {episode_steps} pasos\")\n",
    "            \n",
    "            # Pausa entre episodios\n",
    "            time.sleep(2)\n",
    "        \n",
    "        demo_env.close()\n",
    "        print(\"\\n‚úÖ Demostraci√≥n completada\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error durante la demostraci√≥n: {e}\")\n",
    "        print(\"üì∫ La visualizaci√≥n puede no estar disponible en este entorno\")\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Intentar ejecutar demostraci√≥n\n",
    "# Nota: Comentar la siguiente l√≠nea si no tienes capacidad de renderizado\n",
    "# demonstrate_agent(episodes=2)\n",
    "\n",
    "print(\"üé¨ Demostraci√≥n de c√≥digo preparada\")\n",
    "print(\"üîß Descomenta la l√≠nea anterior para ejecutar la demostraci√≥n visual\")\n",
    "print(\"üì∫ Nota: La demostraci√≥n visual requiere un entorno con GUI disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 An√°lisis Comparativo de Rendimiento\n",
    "\n",
    "Comparemos el rendimiento del agente SAC entrenado con diferentes m√©tricas y benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_random_vs_trained():\n",
    "    \"\"\"\n",
    "    Compara el rendimiento del agente entrenado vs un agente aleatorio\n",
    "    \"\"\"\n",
    "    print(\"‚öñÔ∏è Comparando agente entrenado vs agente aleatorio...\")\n",
    "    \n",
    "    # Crear entorno para comparaci√≥n\n",
    "    comp_env = ShadowHandReachEnvironment(\n",
    "        render_mode=None,\n",
    "        seed=SEED + 5000,\n",
    "        max_episode_steps=50,\n",
    "        reward_type=\"dense\"\n",
    "    )\n",
    "    \n",
    "    # Agente aleatorio\n",
    "    print(\"\\nüé≤ Evaluando agente aleatorio...\")\n",
    "    random_rewards = []\n",
    "    random_successes = []\n",
    "    random_distances = []\n",
    "    \n",
    "    for _ in range(10):\n",
    "        obs, _ = comp_env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < 50:\n",
    "            # Acci√≥n aleatoria\n",
    "            action = comp_env.action_space.sample()\n",
    "            obs, reward, terminated, truncated, info = comp_env.step(action)\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            done = terminated or truncated\n",
    "        \n",
    "        random_rewards.append(episode_reward)\n",
    "        random_successes.append(comp_env.get_success_rate(obs))\n",
    "        random_distances.append(comp_env.get_goal_distance(obs))\n",
    "    \n",
    "    # Agente entrenado\n",
    "    print(\"ü§ñ Evaluando agente entrenado...\")\n",
    "    trained_rewards = []\n",
    "    trained_successes = []\n",
    "    trained_distances = []\n",
    "    \n",
    "    try:\n",
    "        model = SAC.load('models/sac_shadow_hand/sac_best.zip', env=SuccessInfoWrapper(comp_env))\n",
    "        \n",
    "        for _ in range(10):\n",
    "            obs, _ = comp_env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            steps = 0\n",
    "            \n",
    "            while not done and steps < 50:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, info = comp_env.step(action)\n",
    "                episode_reward += reward\n",
    "                steps += 1\n",
    "                done = terminated or truncated\n",
    "            \n",
    "            trained_rewards.append(episode_reward)\n",
    "            trained_successes.append(comp_env.get_success_rate(obs))\n",
    "            trained_distances.append(comp_env.get_goal_distance(obs))\n",
    "        \n",
    "        # An√°lisis comparativo\n",
    "        print(\"\\nüìä Resultados Comparativos:\")\n",
    "        print(f\"\\nüé≤ Agente Aleatorio:\")\n",
    "        print(f\"  üìà Recompensa promedio: {np.mean(random_rewards):.2f} ¬± {np.std(random_rewards):.2f}\")\n",
    "        print(f\"  üéØ Tasa de √©xito promedio: {np.mean(random_successes):.3f}\")\n",
    "        print(f\"  üìè Distancia promedio: {np.mean(random_distances):.4f}\")\n",
    "        \n",
    "        print(f\"\\nü§ñ Agente Entrenado (SAC):\")\n",
    "        print(f\"  üìà Recompensa promedio: {np.mean(trained_rewards):.2f} ¬± {np.std(trained_rewards):.2f}\")\n",
    "        print(f\"  üéØ Tasa de √©xito promedio: {np.mean(trained_successes):.3f}\")\n",
    "        print(f\"  üìè Distancia promedio: {np.mean(trained_distances):.4f}\")\n",
    "        \n",
    "        # Calcular mejora\n",
    "        reward_improvement = np.mean(trained_rewards) - np.mean(random_rewards)\n",
    "        success_improvement = np.mean(trained_successes) - np.mean(random_successes)\n",
    "        distance_improvement = np.mean(random_distances) - np.mean(trained_distances)  # Mejor si es menor\n",
    "        \n",
    "        print(f\"\\nüöÄ Mejoras del Agente Entrenado:\")\n",
    "        print(f\"  üìà Mejora en recompensa: +{reward_improvement:.2f} puntos\")\n",
    "        print(f\"  üéØ Mejora en tasa de √©xito: +{success_improvement:.3f}\")\n",
    "        print(f\"  üìè Mejora en distancia: -{distance_improvement:.4f} (menor es mejor)\")\n",
    "        \n",
    "        if np.mean(random_rewards) != 0:\n",
    "            print(f\"  üî• Factor de mejora en recompensa: {np.mean(trained_rewards)/np.mean(random_rewards):.1f}x\")\n",
    "        \n",
    "        # Visualizaci√≥n\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Comparaci√≥n de recompensas\n",
    "        ax1.boxplot([random_rewards, trained_rewards], labels=['Aleatorio', 'SAC Entrenado'])\n",
    "        ax1.set_ylabel('Recompensa')\n",
    "        ax1.set_title('Comparaci√≥n de Recompensas')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Comparaci√≥n de tasas de √©xito\n",
    "        ax2.boxplot([random_successes, trained_successes], labels=['Aleatorio', 'SAC Entrenado'])\n",
    "        ax2.set_ylabel('Tasa de √âxito')\n",
    "        ax2.set_title('Comparaci√≥n de Tasas de √âxito')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Comparaci√≥n de distancias\n",
    "        ax3.boxplot([random_distances, trained_distances], labels=['Aleatorio', 'SAC Entrenado'])\n",
    "        ax3.set_ylabel('Distancia al Objetivo')\n",
    "        ax3.set_title('Comparaci√≥n de Distancias')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå No se encontr√≥ el modelo entrenado para comparaci√≥n\")\n",
    "    \n",
    "    comp_env.close()\n",
    "\n",
    "# Ejecutar comparaci√≥n\n",
    "compare_random_vs_trained()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 Conclusiones y An√°lisis de Resultados\n",
    "\n",
    "### 11.1 Resumen del Experimento\n",
    "\n",
    "En este experimento implementamos y entrenamos un agente Soft Actor-Critic (SAC) para resolver la tarea de manipulaci√≥n rob√≥tica Shadow Dexterous Hand Reach utilizando entrenamiento paralelo con m√∫ltiples entornos.\n",
    "\n",
    "#### 11.1.1 Configuraci√≥n Experimental\n",
    "\n",
    "**Algoritmo:** Soft Actor-Critic (SAC)\n",
    "- **Justificaci√≥n:** √ìptimo para espacios de acci√≥n continuos de alta dimensionalidad (20 articulaciones)\n",
    "- **Ventajas clave:** Maximizaci√≥n de entrop√≠a, estabilidad off-policy, robustez a hiperpar√°metros\n",
    "\n",
    "**Configuraci√≥n de Entrenamiento:**\n",
    "- **Entornos paralelos:** 4 (aceleraci√≥n te√≥rica 4x)\n",
    "- **Vectorizaci√≥n:** SubprocVecEnv para verdadero paralelismo\n",
    "- **Total de timesteps:** ~200,000 (1000 episodios √ó 50 pasos √ó 4 entornos)\n",
    "- **Arquitectura de red:** 512√ó512√ó256 para actor y cr√≠tico\n",
    "- **Recompensa:** Densa (facilitando el aprendizaje gradual)\n",
    "\n",
    "**Hiperpar√°metros Optimizados:**\n",
    "- **Learning rate:** 3e-4 (est√°ndar para tareas rob√≥ticas)\n",
    "- **Buffer size:** 1,000,000 (retenci√≥n de experiencias diversas)\n",
    "- **Batch size:** 256 (balance estabilidad-eficiencia)\n",
    "- **Gamma:** 0.99 (horizonte largo para manipulaci√≥n)\n",
    "\n",
    "### 11.2 An√°lisis de Resultados\n",
    "\n",
    "#### 11.2.1 Rendimiento del Agente Entrenado\n",
    "\n",
    "Bas√°ndose en las m√©tricas de entrenamiento y evaluaci√≥n observadas:\n",
    "\n",
    "1. **Progreso de Aprendizaje:**\n",
    "   ‚Ä¢ El agente muestra una curva de aprendizaje t√≠pica con exploraci√≥n inicial seguida de mejora gradual\n",
    "   ‚Ä¢ Las recompensas evolucionan desde valores negativos iniciales hacia valores positivos conforme aprende la tarea\n",
    "   ‚Ä¢ La variabilidad se reduce con el tiempo, indicando convergencia hacia una pol√≠tica estable\n",
    "\n",
    "2. **Tasa de √âxito:**\n",
    "   ‚Ä¢ Progreso gradual desde tasa de √©xito inicial baja (~0.0) hacia valores m√°s altos\n",
    "   ‚Ä¢ La tarea de alcanzar objetivos con m√∫ltiples dedos requiere coordinaci√≥n compleja\n",
    "   ‚Ä¢ Mejoras consistentes indican que el agente aprende la coordinaci√≥n requerida\n",
    "\n",
    "3. **Precisi√≥n (Distancia al Objetivo):**\n",
    "   ‚Ä¢ Reducci√≥n progresiva en la distancia promedio a los objetivos\n",
    "   ‚Ä¢ Correlaci√≥n negativa entre distancia y tasa de √©xito\n",
    "   ‚Ä¢ Indica mejora en la precisi√≥n del control de m√∫ltiples articulaciones\n",
    "\n",
    "4. **Eficiencia:**\n",
    "   ‚Ä¢ El entrenamiento paralelo acelera significativamente el proceso de aprendizaje\n",
    "   ‚Ä¢ La utilizaci√≥n de m√∫ltiples entornos permite mayor diversidad de experiencias\n",
    "\n",
    "#### 11.2.2 Comparaci√≥n con Agente Aleatorio\n",
    "\n",
    "El an√°lisis comparativo demuestra la efectividad del aprendizaje:\n",
    "- **Mejora sustancial en recompensas:** El agente entrenado supera significativamente al agente aleatorio\n",
    "- **Tasa de √©xito superior:** Capacidad demostrada para completar la tarea vs. desempe√±o aleatorio negligible\n",
    "- **Precisi√≥n mejorada:** Menor distancia promedio a los objetivos\n",
    "- **Consistencia:** Menor variabilidad en el rendimiento, indicando pol√≠tica robusta\n",
    "\n",
    "### 11.3 Evaluaci√≥n de la Metodolog√≠a\n",
    "\n",
    "#### 11.3.1 Fortalezas del Enfoque\n",
    "\n",
    "1. **Algoritmo Apropiado:**\n",
    "   ‚Ä¢ SAC es ideal para manipulaci√≥n rob√≥tica con espacios de acci√≥n continuos\n",
    "   ‚Ä¢ La maximizaci√≥n de entrop√≠a promueve exploraci√≥n natural en tareas complejas\n",
    "   ‚Ä¢ Robustez a hiperpar√°metros reduce necesidad de ajuste fino extensivo\n",
    "\n",
    "2. **Entrenamiento Paralelo Eficiente:**\n",
    "   ‚Ä¢ Aceleraci√≥n significativa del proceso de aprendizaje\n",
    "   ‚Ä¢ Mayor diversidad de experiencias de entrenamiento\n",
    "   ‚Ä¢ Mejor utilizaci√≥n de recursos computacionales\n",
    "\n",
    "3. **Configuraci√≥n de Recompensa Densa:**\n",
    "   ‚Ä¢ Facilita el aprendizaje gradual vs. recompensa sparse\n",
    "   ‚Ä¢ Proporciona se√±ales de gu√≠a durante la exploraci√≥n inicial\n",
    "   ‚Ä¢ Acelera la convergencia hacia comportamientos deseados\n",
    "\n",
    "#### 11.3.2 Limitaciones y √Åreas de Mejora\n",
    "\n",
    "1. **Tiempo de Entrenamiento:**\n",
    "   ‚Ä¢ Las tareas de manipulaci√≥n rob√≥tica requieren entrenamientos extensos\n",
    "   ‚Ä¢ Podr√≠a beneficiarse de t√©cnicas como pre-entrenamiento o transfer learning\n",
    "\n",
    "2. **Generalizaci√≥n:**\n",
    "   ‚Ä¢ Evaluaci√≥n limitada a configuraciones similares a entrenamiento\n",
    "   ‚Ä¢ Necesidad de evaluar robustez a variaciones en el entorno\n",
    "\n",
    "3. **Complejidad de la Tarea:**\n",
    "   ‚Ä¢ La coordinaci√≥n de m√∫ltiples dedos presenta desaf√≠os inherentes\n",
    "   ‚Ä¢ Requerimientos de precisi√≥n motora fina para alcanzar m√∫ltiples objetivos\n",
    "\n",
    "### 11.4 Conclusiones Principales\n",
    "\n",
    "#### 11.4.1 Capacidades del Agente Implementado\n",
    "\n",
    "1. **Aprendizaje Exitoso:** El agente SAC demuestra capacidad para aprender la tarea compleja de coordinar una mano rob√≥tica de 5 dedos para alcanzar m√∫ltiples objetivos.\n",
    "\n",
    "2. **Mejora Sustancial:** Progreso significativo desde comportamiento aleatorio inicial hacia pol√≠ticas competentes.\n",
    "\n",
    "3. **Estabilidad:** Convergencia hacia pol√≠ticas consistentes y robustas.\n",
    "\n",
    "4. **Eficiencia Computacional:** El entrenamiento paralelo demuestra ser efectivo para acelerar el aprendizaje.\n",
    "\n",
    "#### 11.4.2 Potencial para Aplicaciones Reales\n",
    "\n",
    "Los resultados sugieren que SAC es un algoritmo promisorio para:\n",
    "- **Tareas de manipulaci√≥n rob√≥tica complejas**\n",
    "- **Sistemas con espacios de acci√≥n de alta dimensionalidad** \n",
    "- **Aplicaciones que requieren control preciso y coordinado**\n",
    "- **Tareas de alcance y posicionamiento en rob√≥tica**\n",
    "\n",
    "### 11.5 Trabajo Futuro\n",
    "\n",
    "#### 11.5.1 Extensiones Recomendadas\n",
    "\n",
    "1. **Entrenamiento Extendido:**\n",
    "   ‚Ä¢ Incrementar significativamente el n√∫mero de episodios (10,000+)\n",
    "   ‚Ä¢ Explorar el potencial completo de aprendizaje del agente\n",
    "\n",
    "2. **T√©cnicas Avanzadas:**\n",
    "   ‚Ä¢ Implementar Hindsight Experience Replay (HER) para mejorar eficiencia de muestreo\n",
    "   ‚Ä¢ Explorar curriculum learning para progresi√≥n gradual de dificultad\n",
    "\n",
    "3. **Evaluaci√≥n Robusta:**\n",
    "   ‚Ä¢ Pruebas con variaciones en configuraciones de objetivos\n",
    "   ‚Ä¢ Evaluaci√≥n de transferencia a tareas similares de manipulaci√≥n\n",
    "\n",
    "4. **Optimizaciones:**\n",
    "   ‚Ä¢ Explorar arquitecturas de red m√°s sofisticadas\n",
    "   ‚Ä¢ Investigar t√©cnicas de regularizaci√≥n para mejorar generalizaci√≥n\n",
    "\n",
    "#### 11.5.2 Impacto Potencial\n",
    "\n",
    "Este trabajo contribuye al avance del aprendizaje por refuerzo en rob√≥tica, espec√≠ficamente en:\n",
    "- **Manipulaci√≥n diestra:** Desarrollo de habilidades motoras finas en robots\n",
    "- **Automatizaci√≥n industrial:** Capacidades para tareas de ensamblaje y manipulaci√≥n\n",
    "- **Rob√≥tica de servicio:** Aplicaciones en entornos humanos\n",
    "- **Pr√≥tesis rob√≥ticas:** Control intuitivo de dispositivos prot√©sicos\n",
    "\n",
    "La implementaci√≥n exitosa de SAC en esta tarea compleja demuestra la viabilidad de aplicar deep reinforcement learning a problemas reales de manipulaci√≥n rob√≥tica, sentando las bases para futuros avances en el campo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 Informaci√≥n T√©cnica y Recursos\n",
    "\n",
    "### 12.1 Resumen de Archivos Generados\n",
    "\n",
    "Este notebook genera los siguientes archivos durante el entrenamiento y evaluaci√≥n:\n",
    "\n",
    "#### 12.1.1 Modelos Entrenados\n",
    "‚Ä¢ `models/sac_shadow_hand/sac_best.zip` - Mejor modelo basado en evaluaciones\n",
    "‚Ä¢ `models/sac_shadow_hand/sac_final.zip` - Modelo al final del entrenamiento\n",
    "‚Ä¢ `models/sac_shadow_hand/sac_checkpoint_*.zip` - Checkpoints peri√≥dicos\n",
    "\n",
    "#### 12.1.2 Datos de Entrenamiento\n",
    "‚Ä¢ `logs/sac_shadow_hand/training_log.csv` - M√©tricas detalladas de entrenamiento\n",
    "‚Ä¢ `logs/sac_shadow_hand/training_history.json` - Historia completa de entrenamiento\n",
    "‚Ä¢ `models/sac_shadow_hand/training_info.json` - Informaci√≥n de configuraci√≥n\n",
    "\n",
    "#### 12.1.3 Logs de TensorBoard\n",
    "‚Ä¢ `runs/sac_shadow_hand/` - Logs para visualizaci√≥n en TensorBoard\n",
    "\n",
    "### 12.2 Comandos √ötiles\n",
    "\n",
    "```bash\n",
    "# Visualizar logs de TensorBoard\n",
    "tensorboard --logdir=runs/sac_shadow_hand\n",
    "\n",
    "# Verificar archivos generados\n",
    "ls -la models/sac_shadow_hand/\n",
    "ls -la logs/sac_shadow_hand/\n",
    "```\n",
    "\n",
    "### 12.3 Requisitos del Sistema\n",
    "\n",
    "‚Ä¢ **RAM:** M√≠nimo 8GB, recomendado 16GB+\n",
    "‚Ä¢ **GPU:** Opcional pero recomendada para acelerar entrenamiento\n",
    "‚Ä¢ **Tiempo de entrenamiento:** 4-8 horas para entrenamiento completo\n",
    "‚Ä¢ **Espacio en disco:** ~1GB para modelos y logs\n",
    "\n",
    "### 12.4 Enlaces y Referencias\n",
    "\n",
    "‚Ä¢ [Gymnasium Robotics - Shadow Hand Reach](https://robotics.farama.org/envs/shadow_dexterous_hand/hand_reach/)\n",
    "‚Ä¢ [Stable Baselines3 - SAC](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html)\n",
    "‚Ä¢ [Paper Original SAC](https://arxiv.org/abs/1801.01290)\n",
    "‚Ä¢ [Shadow Dexterous Hand](https://www.shadowrobot.com/dexterous-hand-series/)\n",
    "\n",
    "---\n",
    "\n",
    "**Fin del Notebook - Shadow Dexterous Hand Reach con SAC** ‚úÖ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}